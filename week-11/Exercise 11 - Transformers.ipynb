{"cells":[{"cell_type":"markdown","metadata":{"id":"4pKMjgAeQO64"},"source":["# Exercise: Vision Transformers"]},{"cell_type":"markdown","metadata":{"id":"0NnAgI07NeRb"},"source":["In this exercise session, we will learn about **Vision Transformers**. The Transformer architecture initially emerged in natural language processing as a solution for processing long texts by capturing long-range dependencies. For instance, this is the typical architecture used by Large Language Models like GPT or Gemini. The Vision Transformer, on the other hand, is a variant of the Transformer tailored to process images instead of sequence-like data. While Vision Transformers can outperform CNN architectures on many benchmarks, they may underperform if the amount of data used during training is insufficient."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QZaWQ7d5P9yh"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","\n","from torchvision.transforms import ToTensor\n","from torchvision.datasets import MNIST"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"0N8iROw2SzjW"},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"V4PEHGe5YoRr"},"source":["Just like in previous weeks, we will once again be working with the MNIST dataset:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"zNBoRin_Ynqb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 60000 train and 10000 valid samples.\n"]}],"source":["batch_size = 128\n","\n","# Dataset and DataLoader\n","dataset_train = MNIST('data', train=True, download=True, transform=ToTensor())\n","dataset_test = MNIST('data', train=False, download=True, transform=ToTensor())\n","dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n","\n","print('Loaded {} train and {} valid samples.'.format(len(dataset_train), len(dataset_test)))"]},{"cell_type":"markdown","metadata":{"id":"vL0cMIqIqZNW"},"source":["# Vision Transformer (ViT)"]},{"cell_type":"markdown","metadata":{"id":"OfX1PHF6qcvG"},"source":["The orignal version of the Vision Transformer (ViT) was introduced in the seminal paper of [Dosovitskiy et al., 2020](https://arxiv.org/abs/2010.11929). It adapts the Transformer architecture to process images for tasks such as image classification. Specifically, it replaces the traditional convolutional layers with self-attention mechanisms, enabling it to capture global dependencies within the image. To accommodate image processing, several additional modules are integrated. Here is an overview ([Bazi et al., 2021](https://www.researchgate.net/publication/348947034_Vision_Transformers_for_Remote_Sensing_Image_Classification)) of the various components of this architecture:\n","\n","![ViT Overview](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tA7xE2dQA_dfzA0Bub5TVw.png)"]},{"cell_type":"markdown","metadata":{"id":"n1-KUjYGNeRf"},"source":["In this exercise, we will cover each of these modules individually to gain a comprehensive understanding of the entire architecture. Don't hesitate to come back to this figure."]},{"cell_type":"markdown","metadata":{"id":"ky3yc4_gZ1Vt"},"source":["## Patchification"]},{"cell_type":"markdown","metadata":{"id":"AjJ86EyoncBZ"},"source":["The Transformer architecture operates by treating tokens as input. In textual contexts, each token typically represents either a word or a subword unit. However, in a visual context, tokens are essentially patches extracted from the images. Therefore, the initial step involves dividing the image into several patches of equal size.\n","In the case of the MNIST dataset, we divide the 28x28 images into 7x7 grids, resulting in 49 patches, each measuring 4x4 pixels. It's important to note that since we're working with black-and-white images, there's only a single channel. Following this division, each patch undergoes individual linear projection.\n","\n","![Patchification](https://miro.medium.com/v2/resize:fit:822/format:webp/1*CFbOxEuvo-Pgq7ETIrt0Eg.png)\n"]},{"cell_type":"markdown","metadata":{"id":"9B8nYfaZNeRg"},"source":["Write the code to generate a list of patches for a given set of images. `n_patches` represents the number of patches in a row, set to 7 in this case."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDyldGO1Tr5t"},"outputs":[],"source":["def patchify(images, n_patches):\n","    n, c, h, w = images.shape\n","\n","    assert h == w # We assume square image.\n","\n","    ### WRITE YOUR CODE HERE\n","\n","    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n","    patch_size = h // n_patches\n","\n","\n","assert patchify(torch.rand(2, 1, 28, 28), 2).shape == (2, 4, 196)\n","assert patchify(torch.rand(16, 3, 56, 56), 7).shape == (16, 49, 192)"]},{"cell_type":"markdown","metadata":{"id":"jIEcnYYYaAFp"},"source":["## Positional encoding\n","\n","The positional encoding brings order to the tokens. Without the position token, we do not know where in the image is a particular token from. While there are many different ways to learn the positional encoding, we refer to the earliest implementationm, which used sinusoidals to encode the location."]},{"cell_type":"markdown","metadata":{"id":"xvfIj1nMn8oS"},"source":["$\n","\\text{{positional_encoding}}(i, 2j) = \\sin\\left(\\frac{{i}}{{10000^{2j/d_{\\text{{model}}}}}}\\right)\n","$\n","\n","$\\text{{positional_encoding}}(i, 2j+1) = \\cos\\left(\\frac{{i}}{{10000^{2j/d_{\\text{{model}}}}}}\\right) $"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISlUhQR9awdQ"},"outputs":[],"source":["def get_positional_embeddings(sequence_length, d):\n","    result = torch.ones(sequence_length, d)\n","    for i in range(sequence_length):\n","        for j in range(d):\n","            ### WRITE YOUR CODE HERE\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWyfUxBNoUOB"},"outputs":[],"source":["plt.imshow(get_positional_embeddings(49, 128).numpy(), cmap=\"hot\", interpolation=\"nearest\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"tlz7MLHka1Lw"},"source":["## Multi-head Self-attention\n","\n","The major component in the transformer is the unit of multi-head self-attention mechanism. The transformer views the encoded representation of the input as a set of **key-value** pairs, (**K, V**), both of dimension *n*. The transformer adopts the scaled dot-product attention: the output is a weighted sum of the values, where the weight assigned to each value is determined by the dot-product of the query with all the keys:\n","\n","\n","Attention(Q, K, V) = Softmax$\\dfrac{QK^T}{\\sqrt{n}}V$\n","\n","With multi-head self attention, we repeat the attention module multiple times in parallel with the same Q,K,V triplet. The independent attention outputs are simply concatenated and linearly transformed into the expected dimensions. I assume the motivation is because ensembling always helps? ;) According to the paper, â€œmulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n","\n","![https://lilianweng.github.io/posts/2018-06-24-attention/multi-head-attention.png](https://lilianweng.github.io/posts/2018-06-24-attention/multi-head-attention.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVIQNB24UNnI"},"outputs":[],"source":["class MyMSA(nn.Module):\n","    def __init__(self, d, n_heads=2):\n","        super(MyMSA, self).__init__()\n","        self.d = d\n","        self.n_heads = n_heads\n","\n","        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n","        d_head = int(d / n_heads)\n","        self.d_head = d_head\n","\n","        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n","\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, sequences):\n","        result = []\n","        for sequence in sequences:\n","            seq_result = []\n","            for head in range(self.n_heads):\n","\n","                # Select the mapping associated to the given head.\n","                q_mapping = self.q_mappings[head]\n","                k_mapping = self.k_mappings[head]\n","                v_mapping = self.v_mappings[head]\n","\n","                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n","\n","                # Map seq to q, k, v.\n","                q, k, v = ### WRITE YOUR CODE HERE\n","\n","                attention =  ### WRITE YOUR CODE HERE\n","                seq_result.append(attention @ v)\n","            result.append(torch.hstack(seq_result))\n","        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"]},{"cell_type":"markdown","metadata":{"id":"P4D6GUEkbQvQ"},"source":["## Vit Block"]},{"cell_type":"markdown","metadata":{"id":"T3ESIhpkpiva"},"source":["LayerNorm is a kind of normalization layer which normalizes across all features for a single element in a batch. Given an input, LayerNorm scales the features by subtracting the mean and dividing by the standard deviation.\n","\n","LayerNorm differs from BatchNorm is the way the statistics are computed. Why BatchNorm scales each feature by computing its statistics across the batch, LayerNorm scales features by computing its statistics across all features for a given element in a batch.\n","\n","More information about various normalisation layers: [YouTube, Yannic Kilcher](https://www.youtube.com/watch?v=l_3zj6HeWUE)\n","\n","\n","![Examples of different normalizations](https://miro.medium.com/v2/resize:fit:656/format:webp/1*xf37Ts0mdBGiS-keOwviOQ.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3etpUQylUG-4"},"outputs":[],"source":["class MyViTBlock(nn.Module):\n","    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n","        super(MyViTBlock, self).__init__()\n","        self.hidden_d = hidden_d\n","        self.n_heads = n_heads\n","\n","        self.norm1 = ### WRITE YOUR CODE HERE using LayerNorm pytorch\n","        self.mhsa = MyMSA(hidden_d, n_heads) ### WRITE YOUR CODE HERE\n","        self.norm2 =### WRITE YOUR CODE HERE using LayerNorm pytorch\n","        self.mlp = nn.Sequential( ### WRITE YOUR CODE HERE\n","            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n","            nn.GELU(),\n","            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n","        )\n","\n","    def forward(self, x):\n","        # Write code for MHSA + residual connection.\n","        out = #\n","        # Write code for MLP(Norm(out)) + residual connection\n","        out = #\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"fER1RNuiba4e"},"source":["## ViT model"]},{"cell_type":"markdown","metadata":{"id":"wzkkFa06NeRj"},"source":["Now we can put all together:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6whTzSDGTjaD"},"outputs":[],"source":["class MyViT(nn.Module):\n","    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n","        super(MyViT, self).__init__()\n","\n","        self.chw = chw # (C, H, W)\n","        self.n_patches = n_patches\n","        self.n_blocks = n_blocks\n","        self.n_heads = n_heads\n","        self.hidden_d = hidden_d\n","\n","        # Input and patches sizes\n","        assert chw[1] % n_patches == 0 # Input shape must be divisible by number of patches\n","        assert chw[2] % n_patches == 0\n","        self.patch_size =  ### WRITE YOUR CODE HERE patch size in terms of height and width\n","\n","        # Linear mapper\n","        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n","        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n","\n","        # Learnable classification token\n","        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n","\n","        # Positional embedding\n","        # HINT: don't forget the classification token\n","        self.positional_embeddings =  ### WRITE YOUR CODE HERE\n","\n","        # Transformer blocks\n","        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n","\n","        # Classification MLP\n","        self.mlp = nn.Sequential(\n","            nn.Linear(self.hidden_d, out_d),\n","            nn.Softmax(dim=-1)\n","        )\n","\n","    def forward(self, images):\n","\n","        n, c, h, w = images.shape\n","\n","        # Divide images into patches.\n","        patches = ### WRITE YOUR CODE HERE\n","\n","        # Map the vector corresponding to each patch to the hidden size dimension.\n","        tokens = self.linear_mapper(patches)\n","\n","        # Add classification token to the tokens.\n","        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n","\n","        # Add positional embedding.\n","        # HINT: use torch.Tensor.repeat(...)\n","        out = ### WRITE YOUR CODE HERE\n","\n","        # Transformer Blocks\n","        for block in self.blocks:\n","            out = block(out)\n","\n","        # Get the classification token only.\n","        out = out[:, 0]\n","\n","        # Map to the output distribution.\n","        out = self.mlp(out)\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LS8Z-Op6eFIB"},"outputs":[],"source":["chw = (1, 28, 28)\n","n_patches = 7\n","n_blocks = 2\n","n_blocks = 2\n","hidden_d = 8\n","n_heads = 2\n","out_d = 10\n","model = MyViT(chw = (1, 28, 28), n_patches=n_patches, n_blocks=n_blocks,\n","              hidden_d=hidden_d, n_heads=n_heads, out_d=out_d)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LFUBlIhP4uQ"},"outputs":[],"source":["def accuracy(x, y):\n","    x = x.detach().cpu().numpy()\n","    y = y.detach().cpu().numpy()\n","    return np.mean(np.argmax(x, axis=1) == y)"]},{"cell_type":"markdown","metadata":{"id":"nfu34Jmiexc0"},"source":["Now you can reuse the code of last week exercise, since the only difference is the architecture. The training procedure is the same."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TErun42dcivY"},"outputs":[],"source":["epochs = 20\n","\n","learning_rate = 0.005\n","optimizer = Adam(model.parameters(), lr=learning_rate)\n","criterion = CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GIkuzUVTQF4"},"outputs":[],"source":["def train_model(model, criterion, optimizer, dataloader_train, dataloader_test, epochs):\n","    \"\"\" Trains the model for the specified number of epochs on the dataset.\n","\n","    Args:\n","        model: The model to train.\n","        criterion: The loss function.\n","        optimizer: The optimizer to use.\n","        dataloader_train: The DataLoader for the training set.\n","        dataloader_test: The DataLoader for the test set.\n","        epochs: The number of epochs to train for.\n","    \"\"\"\n","    for ep in range(epochs):\n","        train_loss = 0.0\n","        for it, batch in enumerate(dataloader_train):\n","\n","            # Load a batch, break it down in images and targets.\n","            x, y = batch\n","\n","            # Run forward pass.\n","            logits = model(x)\n","\n","            # Compute loss (using 'criterion').\n","            loss = criterion(logits, y)\n","\n","            # Run backward pass.\n","            loss.backward()\n","\n","            train_loss += loss.detach().cpu().item() / len(dataloader_train)\n","\n","            # Update the weights using 'optimizer'.\n","            optimizer.step()\n","\n","            # Zero-out the accumulated gradients.\n","            optimizer.zero_grad()\n","\n","            print('\\rEp {}/{}, it {}/{}: loss train: {:.2f}, accuracy train: {:.2f}'.\n","                          format(ep + 1, epochs, it + 1, len(dataloader_train), loss,\n","                                accuracy(logits, y)), end='')\n","\n","        # Validation.\n","        model.eval()\n","        with torch.no_grad():\n","            acc_run = 0\n","            for it, batch in enumerate(dataloader_test):\n","                # Get batch of data.\n","                x, y = batch\n","                curr_bs = x.shape[0]\n","                acc_run += accuracy(model(x), y) * curr_bs\n","            acc = acc_run / len(dataloader_test.dataset)\n","\n","            print(', accuracy test: {:.2f}'.format(acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hP4fXPgRUYTp"},"outputs":[],"source":["train_model(model, criterion, optimizer, dataloader_train, dataloader_test, epochs)"]},{"cell_type":"markdown","metadata":{"id":"_sXBRvW-NeRk"},"source":["How does it compare with the previous project on CNNs?\n","\n","In general Transformers are shown to be effective since they leverage large models with a higher parameter count more effectively than the convolutional architectures. However, the performance gains of transformers may not necessarily replicate when comparing against a small convolutional model. Moreover, this is still an active direction of research, with ConvNext and hybrid approaches claiming at par performance with Transformers.\n","\n","Can you think of different hyperparameters to improve model performance?\n","How would the performance change by modifying the number of heads in multi-head attention? How would performance change by altering the number of patches? Did you try alternate positional encoding techniques?\n","\n","Feel free to tinker with this code!"]},{"cell_type":"markdown","metadata":{"id":"4dmvmk7EOp3b"},"source":["These coding exercises were inspired by this [blog post](https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c)."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/zbirobin/iml-week11/blob/main/exercises.ipynb","timestamp":1715113212813}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
