{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Exercise 3: $k$-NN Classifier - Solutions\n",
    "\n",
    "In this exercise you will implement  the **$k$-Nearest Neighbor classifier ($k$-NN)**. You will also get familiar with\n",
    "other very important concepts related to machine learning in practice,\n",
    "including data preprocessing, distance metrics, visualization, and model evaluation.\n",
    "\n",
    "We have provided general functionality and pointers for you here. Please complete the code with your own implementation below. Please also discuss and answer the follow-up questions.\n",
    "\n",
    "### 1. Dataset and problem description\n",
    "\n",
    "The Healthy Body dataset contains body measurements acquired from **1250 people _from different ages, genders, and nationalities_** from different hospitals around the world. Health professionals have performed medical examinations and classified the individuals into three different body categories: **underweight, normal weight, and overweight.**\n",
    "\n",
    "Our goal is to automate the role of the health professionals i.e, to predict the category of the new data . However, due to anonymity reasons, we have been provided access to limited information about the individuals: their measured _weights_ and _heights_, and their respective _body category_ only.\n",
    "\n",
    "We will use these features to train a $k$-NN classifier for the task.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Enable interactive plots, so you can zoom/pan/resize plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for numerical handling and visualization\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data loading and visualization\n",
    "\n",
    "The goal of supervised classification algorithms such as $k$-NN is to use information from a set of labeled examples, i.e., examples for which we know their class assignments, to infer the classes for unlabeled examples, i.e., test examples for which we don't know their class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Weights, heights of individuals and their associated body category \n",
    "features_annotated_path = \"./data/hbody_feats_annotated.npy\"     \n",
    "labels_annotated_path   = \"./data/hbody_labels_annotated.npy\"      \n",
    "\n",
    "# Weights and heights of  individuals with unknown body category \n",
    "# Task: Figure out their body category label using k-NN classifier\n",
    "features_unannotated_path = \"./data/hbody_feats_unannotated.npy\" \n",
    "\n",
    "# ground-truth body categories of  individuals with unknown body category  \n",
    "# to evaluate the k-NN classifier\n",
    "labels_unannotated_path = \"./data/hbody_labels_unannotated_secret.npy\"     \n",
    "\n",
    "# Features organized in an NxD matrix: N examples and D features.\n",
    "# Another way to look at it: each of the N examples is a D-dimensional feature vector.\n",
    "\n",
    "data_train   = np.load(features_annotated_path)\n",
    "data_test    = np.load(features_unannotated_path)\n",
    "labels_train = np.load(labels_annotated_path)\n",
    "labels_test  = np.load(labels_unannotated_path)\n",
    "\n",
    "class_names = ('Underweight', 'Normal weight', 'Overweight')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q1. What are our labels? What are the features that we use to predict these labels?**\n",
    "\n",
    "**Answer**: \n",
    "* Labels: underweight, normal weight, overweight\n",
    "* Features: height and weight\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Data Summary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Number of examples in the training set  : {}\".format(data_train.shape[0]))\n",
    "print(\"Number of examples in the test set      : {}\".format(data_test.shape[0]))\n",
    "\n",
    "plt.bar(class_names, np.bincount(labels_train))\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram plot of each body category in the training set');"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Data visualization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "colors = np.array([[1.0, 0.0, 0.0], \n",
    "                   [0.0, 1.0, 0.0], \n",
    "                   [0.0, 0.0, 1.0]])\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "# visualize the training set\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(f\"Training set ({len(labels_train)} examples)\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*data_train[labels_train==i].T,\n",
    "                c=colors[i, None], alpha=1.0, \n",
    "                s=20, lw=0, label=class_name)\n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.ylabel(\"Height (cm)\")\n",
    "plt.legend();\n",
    "\n",
    "# visualize the test set\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(f\"Test set ({len(labels_test)} examples)\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*data_test[labels_test==i].T,\n",
    "                c=colors[i, None], alpha=1.0, \n",
    "                s=20, lw=0, label=class_name)\n",
    "    \n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.ylabel(\"Height (cm)\")\n",
    "plt.legend();\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q2. Do you think this is an easy or a difficult classification problem? Why?**  \n",
    "\n",
    "**Answer:** Except for amibiguities at the class boundaries, the classes are separated from each other, and learning to separate the classes will be easy or difficult depending on how much mis-classification we would like to allow.\n",
    "\n",
    "**Q3. What should the test set share in common with the training set?**  \n",
    "\n",
    "**Answer:** Both should come from a similar data distribution.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Normalizing the data\n",
    "\n",
    "k-NN determines neighbors by computing the \"distance\" between two examples. For this process to work, we are required to normalize the features. This is true for many other machine learning algorithms as well!\n",
    "\n",
    "**Q. What would happen if we don't do this?**  \n",
    "**A.** Different features will have a different range of values. For example, if we have a feature corresponding to the salary of a person vs his/her weight. The salary will be in the order of $10^5$ whereas the weight will be less than $10^2$. This means that the salary will mostly dominate the distances. Furthermore, without proper normalization, iterative algorithms might take more time to converge.\n",
    "\n",
    "A common way to normalize the data is by the so-called z-score standardization. It transforms values from an arbitrary range such that the results have mean $0$ and standard deviation $1$. \n",
    "\n",
    "The operation is defined as follows:\n",
    "\n",
    "$$\n",
    "x_{norm} = \\frac {x - \\mu_x} {\\sigma_x},\n",
    "$$\n",
    "\n",
    "and is computed _independently for each feature_ $x$ using its mean $\\mu_x$ and standard deviation $\\sigma_x$.\n",
    "\n",
    "Thanks to NumPy however, we can parallelize this by operating on arrays. Pay attention to the dimensions below, taking care that the full data matrix $X$ is $N\\times D$ ($N$ rows of $D$-dimensional vectors)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def normalize(data, means, stds):\n",
    "    \"\"\"This function takes the data, the means,\n",
    "    and the standard deviatons (precomputed), and \n",
    "    returns the normalized data.\n",
    "    \n",
    "    Inputs:\n",
    "        data : shape (NxD)\n",
    "        means: shape (1XD)\n",
    "        stds : shape (1xD)\n",
    "        \n",
    "    Outputs:\n",
    "        normalized data: shape (NxD)\n",
    "    \"\"\"\n",
    "    # return the normalized features\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return (data - means) / stds\n",
    "\n",
    "# test the  normalize function \n",
    "dummy_features = np.random.randint(100,size=(10,3))\n",
    "means = dummy_features.mean(0,keepdims=True)\n",
    "stds  = dummy_features.std(0,keepdims=True)\n",
    "dummy_features_normed = normalize(dummy_features, means, stds)\n",
    "\n",
    "if (np.allclose(dummy_features_normed.mean(axis=0), 0) and \n",
    "    np.allclose(dummy_features_normed.std(axis=0), 1)):\n",
    "    print(\"Everything alright here!!\")\n",
    "else:\n",
    "    print(\"Try again.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. The $k$-Nearest Neighbors Classifier\n",
    "\n",
    "The k-NN classifier assigns the most common label among its k-nearest neighbors for a given example. The method is very intuitive and can be summarized as:\n",
    "- Compute the distance between the example to classify and all the training examples.\n",
    "- Select the closest $k$ training examples.\n",
    "- Assign to the example the most common label among those neighbors.\n",
    "\n",
    "### 4.1 Distance metrics\n",
    "\n",
    "There are many ways to define a distance between two examples. You are probably used to a very common distance metric, the Euclidean distance. \n",
    "\n",
    "#### Euclidean distance:\n",
    "\n",
    "$$\n",
    "D(\\mathbf{v}, \\mathbf{w}) = \\sqrt{ \\sum_{i=1}^D \\left(\\mathbf{v}^{(i)} - \\mathbf{w}^{(i)}\\right)^2 }\n",
    "$$\n",
    "\n",
    "This distance metric corresponds to our intuitive interpretation of the straight-line distance between two vectors $\\mathbf{v}\\in\\mathbf{R}^D$ and $\\mathbf{w}\\in\\mathbf{R}^D$. Note that $\\mathbf{v}^{(i)}$ denotes the value in the dimension $i$ of $\\mathbf{v}$.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def euclidean_dist(example, training_examples):\n",
    "    \"\"\"Compute the Euclidean distance between a single example\n",
    "    vector and all training_examples.\n",
    "\n",
    "    Inputs:\n",
    "        example: shape (D,)\n",
    "        training_examples: shape (NxD) \n",
    "    Outputs:\n",
    "        euclidean distances: shape (N,)\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return np.sqrt(((training_examples - example) ** 2).sum(axis=1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def find_k_nearest_neighbors(k, distances):\n",
    "    \"\"\" Find the indices of the k smallest distances from a list of distances.\n",
    "        Tip: use np.argsort()\n",
    "\n",
    "    Inputs:\n",
    "        k: integer\n",
    "        distances: shape (N,) \n",
    "    Outputs:\n",
    "        indices of the k nearest neighbors: shape (k,)\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    indices = np.argsort(distances)[:k]\n",
    "    return indices\n",
    "\n",
    "# test the find_k_nearest_neighbors function\n",
    "dummy_distances = [10., 0.5, 200, 0.006, 43, 4.5, 11., 50]\n",
    "top_k = 5\n",
    "top_k_indices = find_k_nearest_neighbors(top_k,dummy_distances)\n",
    "\n",
    "if np.allclose(top_k_indices,[3,1,5,0,6]):\n",
    "    print('Implementation is correct')\n",
    "else:\n",
    "    print('Oops!! Something is wrong')\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Given a list of neighbor labels, choose the most frequent one.\n",
    "# Tip: np.bincount and np.argmax are your friend.\n",
    "\n",
    "def predict_label(neighbor_labels):\n",
    "    \"\"\"Return the most frequent label in the neighbors'.\n",
    "\n",
    "    Inputs:\n",
    "        neighbor_labels: shape (N,) \n",
    "    Outputs:\n",
    "        most frequent label\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return np.argmax(np.bincount(neighbor_labels))\n",
    "\n",
    "# test label prediction\n",
    "dummy_labels = [10, 3, 2, 10, 2, 2, 2, 10, 10, 11, 1, 2]\n",
    "\n",
    "if predict_label(dummy_labels) == 2:\n",
    "    print('Implementation is correct')\n",
    "else:\n",
    "    print('Oops!! Something is wrong')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 $k$-NN classifier, step by step for a single test example\n",
    "\n",
    "Let's implement the algorithm for **one data sample**, i.e., to classify a single new point.\n",
    "\n",
    "**Q4. You are asked to use the mean and std of the training set to normalize the test data. Can you explain the reasoning behind this?** \n",
    "\n",
    "**Answer** We are not supposed to know anything about the test set during the training phase. Therefore, we use training set's mean and std to normalize the entire dataset, including test data. We can do this since we assume that the statistics of the training and test are the same, meaning that the samples are drawn from the same distribution.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def choose_random_sample(data):\n",
    "    \"\"\"Randomly chose a single datapoint from the given data matrix.\n",
    "    \n",
    "    Input:\n",
    "        data: shape (NxD)\n",
    "    Output:\n",
    "        randomly chosen sample: shape (1xD)\n",
    "    \"\"\"\n",
    "    total_samples = data.shape[0]\n",
    "    rand_idx = np.random.choice(total_samples)\n",
    "    return data[rand_idx : rand_idx + 1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Use the functions you defined above to predict the label of a single example-\n",
    "\n",
    "# First, we choose a random test example \n",
    "single_test_data =  choose_random_sample(data_test)\n",
    "\n",
    "# IMPORTANT: Normalize the data, what should the mean and std be?\n",
    "# Hint: mean_val and std_val dimension should be (1x2). Make use of reshape or the argument `keepdims`.\n",
    "\n",
    "# WRITE YOUR CODE HERE\n",
    "mean_val = data_train.mean(axis=0, keepdims=True)\n",
    "std_val = data_train.std(axis=0, keepdims=True)\n",
    "\n",
    "# norm_train is the normalized data_train\n",
    "norm_train = normalize(data_train, mean_val, std_val)\n",
    "\n",
    "# norm_test_single is the normalized single_test_data\n",
    "norm_test_single = normalize(single_test_data, mean_val, std_val)\n",
    "\n",
    "# set the number of neighbors in the kNN classifier (hyperparameter)\n",
    "# (you can play with that)\n",
    "k = 6\n",
    "\n",
    "# reshape the single example from (1xD) to (D,) for the next function\n",
    "norm_test_single = norm_test_single.reshape(-1)\n",
    "# find distance of the single test example w.r.t. all training examples\n",
    "distances = euclidean_dist(norm_test_single, norm_train)\n",
    "\n",
    "# find the nearest neighbors\n",
    "nn_indices = find_k_nearest_neighbors(k, distances)\n",
    "\n",
    "# find the labels of the nearest neighbors\n",
    "neighbor_labels = labels_train[nn_indices] \n",
    "\n",
    "print(\"Nearest {} neighbors' labels: {}\\n\".format(k, \", \".join([class_names[x] for x in neighbor_labels])))\n",
    "\n",
    "# find the best label\n",
    "best_label = predict_label(neighbor_labels) \n",
    "\n",
    "print(f'Predicted label: {class_names[best_label]}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The cell below is for visualization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the unknown point and its neighbors.\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.title(f\"A randomly chosen unlabeled example from the \"\n",
    "          \"test set\\nand its k-nearest neighbors\")\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*norm_train[labels_train==i].T,\n",
    "                c=colors[i, None], alpha=0.25, \n",
    "                s=20, lw=0, label=class_name)\n",
    "    \n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_indices = nn_indices[labels_train[nn_indices] == i]\n",
    "    if len(class_indices) > 0:\n",
    "        plt.scatter(*norm_train[class_indices].T,\n",
    "                    c=colors[i, None], alpha=1, \n",
    "                    s=25, lw=0, label='Neighbor')\n",
    "\n",
    "# Make sure the norm_test is 1D vector.\n",
    "plt.scatter(*norm_test_single.reshape((-1)), marker='*', c='darkorange', \n",
    "                 alpha=.9, s=75, label='unlabeled example')\n",
    "\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.legend();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.3 Performance metrics\n",
    "\n",
    "To quantify the performance of our model, we want to obtain a score that tells us how close the predictions were to the expected classification.\n",
    "\n",
    "The simplest way to do this is to compute the ratio of correctly predicted examples, also known as the accuracy:\n",
    "\n",
    "$$\n",
    "\\frac 1 N \\sum_{n=1}^N \\mathbf{1}[\\hat{y} = y]\n",
    "$$\n",
    "\n",
    "where the indicator function $\\mathbf{1}[\\hat{y} = y]$ returns 1 if the predicted $\\hat{y}$ is equal to the ground-truth $y$ and 0 otherwise.\n",
    "\n",
    "**Q5. Do you see any limitation to using accuracy to evaluate your model?**\n",
    "\n",
    "**Answer.** By using accuracy only, we do not care what particular mistakes the model made. One can argue that incorrectly labeling an overweight person as underweight is worse than labeling them as normal weight. Also, if most people belonged to one class, just predicting that class for the whole population would give a very high accuracy. Plain accuracy does not account for class imbalances.\n",
    "\n",
    "**Q6. Can you think of other ways to evaluate your model?**\n",
    "\n",
    "**Answer.** There are many. Qualitatively, one could inspect what mistakes the model made, by computing a confusion matrix: a matrix counting all the predictions and expected labels. One could improve the accuracy metric by assigning weights to each example related to its class frequency: the more uncommon a class is, the more important it is that the model classifies correctly the examples belonging to such class.\n",
    "\n",
    "**Q7. What other criteria, aside from accuracy, should one consider when choosing hyperparameters?**  \n",
    "\n",
    "**Answer.** One other criterion is model complexity. When possible, one should choose simpler models over more complicated ones. Other criteria include robustness, speed, and scalability. \n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write a function that computes the accuracy between a predicted and the expected labels.\n",
    "def compute_accuracy(predicted, target):\n",
    "    \"\"\"Returns the accuracy score.\n",
    "\n",
    "    Inputs:\n",
    "        predicted: shape (N,) \n",
    "        target: shape (N,) \n",
    "    Outputs:\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return np.mean(predicted == target)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.4 Putting things together to run the k-NN classifier for all examples\n",
    "\n",
    "Let's implement the algorithm for **all data samples**.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write a function kNN_one_example that applies all the previous steps\n",
    "# to predict the label of 'one' example.\n",
    "\n",
    "def kNN_one_example(unlabeled_example, training_features, training_labels, k):\n",
    "    \"\"\"Returns the label of a single unlabelled example.\n",
    "\n",
    "    Inputs:\n",
    "        unlabeled_example: shape (D,) \n",
    "        training_features: shape (NxD)\n",
    "        training_labels: shape (N,) \n",
    "        k: integer\n",
    "    Outputs:\n",
    "        predicted label\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    \n",
    "    # Compute distances\n",
    "    distances = euclidean_dist(unlabeled_example, training_features) \n",
    "    \n",
    "    # Find neighbors\n",
    "    nn_indices = find_k_nearest_neighbors(k, distances)    \n",
    "    \n",
    "    # Get neighbors' labels\n",
    "    neighbor_labels = training_labels[nn_indices] \n",
    "    \n",
    "    # Pick the most common\n",
    "    best_label = predict_label(neighbor_labels)\n",
    "    \n",
    "    return best_label"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Write a function kNN that applies 'kNN_one_example' function to an arbitrary number of examples.\n",
    "# Tip: NumPy's apply_along_axis does most of the work for you. \n",
    "# It's a one-liner, but you might need to read its documentation!\n",
    "\n",
    "def kNN(unlabeled, training_features, training_labels, k):\n",
    "    \"\"\"Return the labels vector for all unlabeled datapoints.\n",
    "\n",
    "    Inputs:\n",
    "        unlabeled: shape (MxD) \n",
    "        training_features: shape (NxD)\n",
    "        training_labels: shape (N,) \n",
    "        k: integer\n",
    "    Outputs:\n",
    "        predicted labels: shape (M,)\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return np.apply_along_axis(func1d=kNN_one_example, axis=1, arr=unlabeled, \n",
    "                               training_features=training_features, \n",
    "                               training_labels=training_labels, k=k)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Normalize your training data and test data\n",
    "# (Don't forget to normalize according to the mean and std of the training set!)\n",
    "\n",
    "mean_val = data_train.mean(axis=0, keepdims=True)\n",
    "std_val  = data_train.std(axis=0, keepdims=True)\n",
    "norm_train_data = normalize(data_train, mean_val, std_val)\n",
    "norm_test_data  = normalize(data_test, mean_val, std_val)\n",
    "\n",
    "# choose a k value (you can play with that)\n",
    "k = 6\n",
    "\n",
    "# run k-NN classifier on complete test data\n",
    "predicted_labels_test = kNN(norm_test_data, norm_train_data, labels_train, k)\n",
    "\n",
    "accuracy = compute_accuracy(predicted_labels_test, labels_test)\n",
    "print(\"Test Accuracy is {:.1f}%\".format(100*accuracy))\n",
    "\n",
    "# Visualize the predictions on the unannotated test set\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Predicted classes for test data\")\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*norm_train_data[labels_train==i].T,\n",
    "                c=colors[i, None], alpha=0.1, s=15, lw=0)\n",
    "    \n",
    "# represent test set by '*' marker\n",
    "for i, class_name in enumerate(class_names):    \n",
    "    plt.scatter(*norm_test_data[predicted_labels_test==i].T,\n",
    "                c=colors[i, None], marker='*', alpha=0.7, \n",
    "                s=50, lw=0, label=class_name)\n",
    "    \n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.legend();"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q8. How do you expect the model to perform with large k values equal to the number of training example ?**\n",
    "\n",
    "**Answer.** For large values of k, the k-NN classifier predicts the label of the most frequent class in the training set.\n",
    "\n",
    "**Q9. While the above implementation works, it has some drawbacks. Can you identify them?**  \n",
    "\n",
    "**Answer.** Since we are calculating the distance of a point w.r.t. all the other points, we will have duplicate computations since D(a,b) = D(b,a) for most of the distance metrics (D) for points (a, b). Additionally, data points are ordered in space and when using a distance metric like the Euclidean one, one have an ordering property. If a point A is further away from point B than point C, then a new point P, if further away from point A, is also further away from point B. Hence, one can skip the distance calculation with w.r.t. P and B, leading to fewer computations.\n",
    "\n",
    "**Q10. What should one take into account when feeding new data to a machine learning model?**\n",
    "\n",
    "**Answer.** The same-distribution assumption. This in practice also means that any transformation we applied on the training data, such as the data normalization operation that we applied above, should be applied in the same way to all the rest of the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5 Written questions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q1.** You use $k$-NN on a dataset with $k=21$. You find out that it overfits as performance on the training set is a lot better than on the validation set. What do you expect might help to reduce overfitting: decreasing or increasing the number of neighbors $k$?\n",
    "\n",
    "**A1.** *Increasing* $k$ might help against overfitting. Indeed, the \"complexity\" of the model has an inversed relation to $k$: the smaller the $k$ the more complex the model, and the larger the $k$ the more simple the model. To understand this, think of the complexity of the boundary decision for $k=1$ and $k=N$ (:= the size of the training set)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q2.** Class imbalance in the training set can be problematic for $k$-NN. Why? What potential solutions could help?\n",
    "\n",
    "**A2.** $k$-NN relies on the _most frequent_ label amongst the neighbors of a point. If the classes are imbalanced, then the frequent class might end up dominating this \"vote\". To solve this, one can try to weight the vote by the inverse of the class size, sub-sampling the dominant classes (i.e., removing some points from the training set), or augmenting the rarer classes by generating synthetic examples."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q3.** (MCQ) Which of the following statements are true for $k$-NN?\n",
    "1. For $k=100$, the time to train $k$-NN is longer than for $k=10$.\n",
    "2. $k$ should be set to the number of classes.\n",
    "3. $k$-NN has _no_ trainable parameters.\n",
    "4. The prediction of the algorithm might change with the distance metric.\n",
    "\n",
    "**A3.** 3 & 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q4.** (SCQ) Let us consider the 2-dimensional training data in the figure below, which contains two classes: \"Positive\" and \"Negative\", denoted by the symbols $+$ and $-$ respectively. Furthermore, let us consider the cosine distance between two points $\\mathrm{u}$ and $\\mathrm{v}$ given by\n",
    "\n",
    "$$ d_{cosine}(\\mathrm{u}, \\mathrm{v}) = 1 - \\frac{\\mathrm{u}^\\top\\mathrm{v}}{\\lVert\\mathrm{u}\\rVert_2 \\cdot \\lVert\\mathrm{v}\\rVert_2} = 1 - \\cos\\theta, $$\n",
    "\n",
    "where $\\theta$ is the angle between the vector $\\mathrm{u}$ and $\\mathrm{v}$.\n",
    "\n",
    "![](img/knn_question.png)\n",
    "\n",
    "Which of the following statement is the correct one if we predict the label of the test point with a $1$-NN?\n",
    "1. Using both cosine and euclidean distances, it is classified as \"Positive\".\n",
    "2. It is classified as \"Positive\" with the cosine distance and \"Negative\" with euclidean distance.\n",
    "3. Using both cosine and euclidean distances, it is classified as \"Negative\".\n",
    "4. It is classified as \"Negative\" with the cosine distance and \"Positive\" with euclidean distance.\n",
    "\n",
    "**A4.** 4"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 Cross Validation (Optional)\n",
    "\n",
    "How should we be choosing the value of k? If we choose the k that gives us the highest **test** accuracy, we would be **cheating**, because we would be tuning our model to its test performance. \n",
    "\n",
    "In practice, we choose the k that gives us the highest **validation** accuracy, via the cross-validation method. By doing so, we ensure that we select a method which generalizes well to unseen test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1.  K-Fold Cross Validation \n",
    "In the course, you saw that we generally reserve a portion of the training data for validation, i.e., testing the performance of our model to choose the hyper-parameter. Below, we introduce \"K-Fold Cross-Validation\".\n",
    "\n",
    "K-fold is a type of cross validation technique that works in the following way:\n",
    "\n",
    "1 - Select a $k$ (do not confuse this with the K-Fold of the cross validation, this is the $k$ for the kNN method!)\n",
    "\n",
    "2 - Split the training data in K equal and disjoint parts\n",
    "\n",
    "3 - Select 1 part as our validation set and the rest as our training set.\n",
    "\n",
    "4 - Train our model on our training set and find the accuracy of the validation set. \n",
    "\n",
    "5 - Repeat steps 3 and 4 K times, each time selecting a different part of the data for the validation set. In the end we will find K different validation accuracies. We will average the validation accuracies and find the validation accuracy that corresponds to the k we chose for kNN. (See the image below).\n",
    "\n",
    "6 - Repeat steps 1-5 (the whole process) for different $k$ values (hyperparameter for kNN). \n",
    "\n",
    "7 - Find the $k$ value that gave the highest validation accuracy. Train your model on the whole training set using this $k$. Test on the test set and report the test accuracy.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q. What is the difference between validation set and test set?**  \n",
    "**A.** The validation set is used during the training process, and helps us make decisions about our model. The test set is not used during training, and does not affect training in any way. It is only used when we have finialized our model, to get an idea of how well our model generalizes to unseen data.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](img/cross_validation.png)\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2. Implementation\n",
    "\n",
    "Now let's begin! We will be doing steps 1-5 of our algorithm above. We will do a K-Fold cross validation to find the validation accuracy for a selected $k$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def KFold_cross_validation_KNN(X, Y, K, k):\n",
    "    '''\n",
    "    K-Fold Cross validation function for K-NN\n",
    "\n",
    "    Inputs:\n",
    "        X : training data, shape (NxD)\n",
    "        Y: training labels, shape (N,)\n",
    "        K: number of folds (K in K-fold)\n",
    "        k: number of neighbors for kNN algorithm (the hyperparameter)\n",
    "    Returns:\n",
    "        Average validation accuracy for the selected k.\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    accuracies = []  # list of accuracies\n",
    "    for fold_ind in range(K):\n",
    "        #Split the data into training and validation folds:\n",
    "        \n",
    "        #all the indices of the training dataset\n",
    "        all_ind = np.arange(N)\n",
    "        split_size = N // K\n",
    "        \n",
    "        val_ind = all_ind[fold_ind * split_size : (fold_ind + 1) * split_size]\n",
    "        ## YOUR CODE HERE (hint: np.setdiff1d is your friend)\n",
    "        train_ind = np.setdiff1d(all_ind, val_ind, assume_unique=True)\n",
    "        \n",
    "        X_train_fold = X[train_ind, :]\n",
    "        Y_train_fold = Y[train_ind]\n",
    "        X_val_fold = X[val_ind, :]\n",
    "        Y_val_fold = Y[val_ind]\n",
    "\n",
    "        # Run KNN using the data folds you found above.\n",
    "        # YOUR CODE HERE\n",
    "        Y_val_fold_pred = kNN(X_val_fold, X_train_fold, Y_train_fold, k)\n",
    "        acc = compute_accuracy(Y_val_fold_pred, Y_val_fold)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    #Find the average validation accuracy over K:\n",
    "    ave_acc = np.mean(accuracies)\n",
    "    return ave_acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Perform a 4-fold cross validation using k-NN, with k=5.\n",
    "\n",
    "K = 4 # 4 fold cross validation\n",
    "k = 5 # number of nearest neighbours for k-NN\n",
    "acc = KFold_cross_validation_KNN(norm_train_data, labels_train, K, k)\n",
    "print(f\"{k}-NN Classifier predicted with average validation accuracy of {acc:.2%}.\")\n",
    "\n",
    "# If your accuracy is not above 80%, you may have made a mistake."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3 Hyperparameter optimization\n",
    "\n",
    "Now let's find the best $k$! Run the function you wrote above for different values of $k$, and let's see which one gives the best validation accuracy.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Call the cross validation method with different k values\n",
    "\n",
    "def run_cv_for_hyperparam(X, Y, K, k_list):\n",
    "    '''\n",
    "    K-Fold Cross validation function for K-NN\n",
    "\n",
    "    Inputs:\n",
    "        X : training data, shape (NxD)\n",
    "        Y: training labels, shape (N,)\n",
    "        K: number of folds (K in K-fold)\n",
    "        k: a list of k values for kNN \n",
    "    Returns:\n",
    "        model_performance: a list of validation accuracies corresponding to the k-values     \n",
    "    '''\n",
    "    model_performance = [] \n",
    "    for k in k_list:\n",
    "        #YOUR CODE HERE\n",
    "        model_performance.append(KFold_cross_validation_KNN(X, Y, K, k))      \n",
    "    return model_performance\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Visualize the performances for different values of k\n",
    "\n",
    "# Try these values for hyperparameter k\n",
    "k_list = range(1, 100, 4)          \n",
    "K = 4 # number of fold for K-Fold\n",
    "\n",
    "## YOUR CODE HERE\n",
    "model_performance= run_cv_for_hyperparam(norm_train_data,labels_train, K, k_list)\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.title(\"Performance on the validation set for different values of $k$\")\n",
    "plt.plot(k_list, model_performance)\n",
    "plt.xlabel(\"Number of nearest neighbors $k$\")\n",
    "plt.xticks(k_list)\n",
    "plt.ylabel(\"Performance (accuracy)\");"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Pick hyperparameter value that yields the best performance\n",
    "# WRITE YOUR CODE HERE\n",
    "best_k = k_list[np.argmax(model_performance)]\n",
    "\n",
    "print(f\"Best number of nearest neighbors on validation set is k={best_k}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.4. Test accuracies for best model.\n",
    "\n",
    "Now that we have tuned our model, we can apply it for prediction on the test set using the optimal $k$ found on cross-validations set.\n",
    "\n",
    "**Q. How do you expect the model to perform, compared with the cross-validation set performance?**\n",
    "\n",
    "**A.** If our validation set at every fold level is representative of the underlying distribution, we can expect similar performance. How accurate they are will depend on the problem. Usually due to sampling bias in the training and validation sets, we can expect slightly lower results on the test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get predicted labeles for unannotated data.\n",
    "# WRITE YOUR CODE HERE\n",
    "predicted_labels_test = kNN(norm_test_data, norm_train_data, labels_train, best_k)\n",
    "accuracy = compute_accuracy(predicted_labels_test, labels_test)\n",
    "print(\"Test Accuracy is {:.1f}%\".format(100*accuracy))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# Visualize the predictions on the unannotated test set\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Predicted classes for test data\")\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*norm_train_data[labels_train==i].T,\n",
    "                c=colors[i, None], alpha=0.1, s=15, lw=0)\n",
    "    \n",
    "# represent test set by '*' marker\n",
    "for i, class_name in enumerate(class_names):    \n",
    "    plt.scatter(*norm_test_data[predicted_labels_test==i].T,\n",
    "                c=colors[i, None], marker='*', alpha=0.7, \n",
    "                s=50, lw=0, label=class_name)\n",
    "    \n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.legend();"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.16 64-bit ('introml': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "interpreter": {
   "hash": "57d8526aca424c3deaf369d9657bf7dc80e220f5e8c5420c16cbdbf3db48769f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}