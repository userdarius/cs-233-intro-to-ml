{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise: $k$-NN Classifier\n",
    "\n",
    "In this exercise you will implement  the **$k$-Nearest Neighbor classifier ($k$-NN)**. You will also get familiar with\n",
    "other very important concepts related to machine learning in practice,\n",
    "including data preprocessing, distance metrics, visualization, and model evaluation.\n",
    "\n",
    "We have provided general functionality and pointers for you here. Please complete the code with your own implementation below. Please also discuss and answer the follow-up questions.\n",
    "\n",
    "### 1. Dataset and problem description\n",
    "\n",
    "The Healthy Body dataset contains body measurements acquired from **1250 people _from different ages, genders, and nationalities_** from different hospitals around the world. Health professionals have performed medical examinations and classified the individuals into three different body categories: **underweight, normal weight, and overweight.**\n",
    "\n",
    "Our goal is to automate the role of the health professionals i.e, to predict the category of the new data . However, due to anonymity reasons, we have been provided access to limited information about the individuals: their measured _weights_ and _heights_, and their respective _body category_ only.\n",
    "\n",
    "We will use these features to train a $k$-NN classifier for the task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable interactive plots, so you can zoom/pan/resize plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Libraries for numerical handling and visualization\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loading and visualization\n",
    "\n",
    "The goal of supervised classification algorithms such as $k$-NN is to use information from a set of labeled examples, i.e., examples for which we know their class assignments, to infer the classes for unlabeled examples, i.e., test examples for which we don't know their class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights, heights of individuals and their associated body category \n",
    "features_annotated_path = \"./data/hbody_feats_annotated.npy\"     \n",
    "labels_annotated_path   = \"./data/hbody_labels_annotated.npy\"      \n",
    "\n",
    "# Weights and heights of  individuals with unknown body category \n",
    "# Task: Figure out their body category label using k-NN classifier\n",
    "features_unannotated_path = \"./data/hbody_feats_unannotated.npy\" \n",
    "\n",
    "# ground-truth body categories of  individuals with unknown body category  \n",
    "# to evaluate the k-NN classifier\n",
    "labels_unannotated_path = \"./data/hbody_labels_unannotated_secret.npy\"     \n",
    "\n",
    "# Features organized in an NxD matrix: N examples and D features.\n",
    "# Another way to look at it: each of the N examples is a D-dimensional feature vector.\n",
    "\n",
    "data_train   = np.load(features_annotated_path)\n",
    "data_test    = np.load(features_unannotated_path)\n",
    "labels_train = np.load(labels_annotated_path)\n",
    "labels_test  = np.load(labels_unannotated_path)\n",
    "\n",
    "class_names = ('Underweight', 'Normal weight', 'Overweight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What are our labels? What are the features that we use to predict these labels?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of examples in the training set  : {}\".format(data_train.shape[0]))\n",
    "print(\"Number of examples in the test set      : {}\".format(data_test.shape[0]))\n",
    "\n",
    "plt.bar(class_names, np.bincount(labels_train))\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram plot of each body category in the training set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([[1.0, 0.0, 0.0], \n",
    "                   [0.0, 1.0, 0.0], \n",
    "                   [0.0, 0.0, 1.0]])\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "# visualize the training set\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(f\"Training set ({len(labels_train)} examples)\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*data_train[labels_train==i].T,\n",
    "                c=colors[i, None], alpha=1.0, \n",
    "                s=20, lw=0, label=class_name)\n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.ylabel(\"Height (cm)\")\n",
    "plt.legend();\n",
    "\n",
    "# visualize the test set\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(f\"Test set ({len(labels_test)} examples)\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*data_test[labels_test==i].T,\n",
    "                c=colors[i, None], alpha=1.0, \n",
    "                s=20, lw=0, label=class_name)\n",
    "    \n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.ylabel(\"Height (cm)\")\n",
    "plt.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. Do you think this is an easy or a difficult classification problem? Why?**  \n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Q3. What should the test set share in common with the training set?**  \n",
    "\n",
    "**Answer:** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalizing the data\n",
    "\n",
    "k-NN determines neighbors by computing the \"distance\" between two examples. For this process to work, we are required to normalize the features. This is true for many other machine learning algorithms as well!\n",
    "\n",
    "**Q. What would happen if we don't do this?**  \n",
    "**A.** \n",
    "\n",
    "A common way to normalize the data is by the so-called z-score standardization. It transforms values from an arbitrary range such that the results have mean $0$ and standard deviation $1$. \n",
    "\n",
    "The operation is defined as follows:\n",
    "\n",
    "$$\n",
    "x_{norm} = \\frac {x - \\mu_x} {\\sigma_x},\n",
    "$$\n",
    "\n",
    "and is computed _independently for each feature_ $x$ using its mean $\\mu_x$ and standard deviation $\\sigma_x$.\n",
    "\n",
    "Thanks to NumPy however, we can parallelize this by operating on arrays. Pay attention to the dimensions below, taking care that the full data matrix $X$ is $N\\times D$ ($N$ rows of $D$-dimensional vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, means, stds):\n",
    "    \"\"\"This function takes the data, the means,\n",
    "    and the standard deviatons (precomputed), and \n",
    "    returns the normalized data.\n",
    "    \n",
    "    Inputs:\n",
    "        data : shape (NxD)\n",
    "        means: shape (1XD)\n",
    "        stds : shape (1xD)\n",
    "        \n",
    "    Outputs:\n",
    "        normalized data: shape (NxD)\n",
    "    \"\"\"\n",
    "    # return the normalized features\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return ...\n",
    "\n",
    "# test the  normalize function \n",
    "dummy_features = np.random.randint(100,size=(10,3))\n",
    "means = dummy_features.mean(0,keepdims=True)\n",
    "stds  = dummy_features.std(0,keepdims=True)\n",
    "dummy_features_normed = normalize(dummy_features, means, stds)\n",
    "\n",
    "if (np.allclose(dummy_features_normed.mean(axis=0), 0) and \n",
    "    np.allclose(dummy_features_normed.std(axis=0), 1)):\n",
    "    print(\"Everything alright here!!\")\n",
    "else:\n",
    "    print(\"Try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The $k$-Nearest Neighbors Classifier\n",
    "\n",
    "The k-NN classifier assigns the most common label among its k-nearest neighbors for a given example. The method is very intuitive and can be summarized as:\n",
    "- Compute the distance between the example to classify and all the training examples.\n",
    "- Select the closest $k$ training examples.\n",
    "- Assign to the example the most common label among those neighbors.\n",
    "\n",
    "### 4.1 Distance metrics\n",
    "\n",
    "There are many ways to define a distance between two examples. You are probably used to a very common distance metric, the Euclidean distance. \n",
    "\n",
    "#### Euclidean distance:\n",
    "\n",
    "$$\n",
    "D(\\mathbf{v}, \\mathbf{w}) = \\sqrt{ \\sum_{i=1}^D \\left(\\mathbf{v}^{(i)} - \\mathbf{w}^{(i)}\\right)^2 }\n",
    "$$\n",
    "\n",
    "This distance metric corresponds to our intuitive interpretation of the straight-line distance between two vectors $\\mathbf{v}\\in\\mathbf{R}^D$ and $\\mathbf{w}\\in\\mathbf{R}^D$. Note that $\\mathbf{v}^{(i)}$ denotes the value in the dimension $i$ of $\\mathbf{v}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(example, training_examples):\n",
    "    \"\"\"Compute the Euclidean distance between a single example\n",
    "    vector and all training_examples.\n",
    "\n",
    "    Inputs:\n",
    "        example: shape (D,)\n",
    "        training_examples: shape (NxD) \n",
    "    Outputs:\n",
    "        euclidean distances: shape (N,)\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nearest_neighbors(k, distances):\n",
    "    \"\"\" Find the indices of the k smallest distances from a list of distances.\n",
    "        Tip: use np.argsort()\n",
    "\n",
    "    Inputs:\n",
    "        k: integer\n",
    "        distances: shape (N,) \n",
    "    Outputs:\n",
    "        indices of the k nearest neighbors: shape (k,)\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    indices = ...\n",
    "    return indices\n",
    "\n",
    "# test the find_k_nearest_neighbors function\n",
    "dummy_distances = [10., 0.5, 200, 0.006, 43, 4.5, 11., 50]\n",
    "top_k = 5\n",
    "top_k_indices = find_k_nearest_neighbors(top_k,dummy_distances)\n",
    "\n",
    "if np.allclose(top_k_indices,[3,1,5,0,6]):\n",
    "    print('Implementation is correct')\n",
    "else:\n",
    "    print('Oops!! Something is wrong')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of neighbor labels, choose the most frequent one.\n",
    "# Tip: np.bincount and np.argmax are your friend.\n",
    "\n",
    "def predict_label(neighbor_labels):\n",
    "    \"\"\"Return the most frequent label in the neighbors'.\n",
    "\n",
    "    Inputs:\n",
    "        neighbor_labels: shape (N,) \n",
    "    Outputs:\n",
    "        most frequent label\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return ...\n",
    "\n",
    "# test label prediction\n",
    "dummy_labels = [10, 3, 2, 10, 2, 2, 2, 10, 10, 11, 1, 2]\n",
    "\n",
    "if predict_label(dummy_labels) == 2:\n",
    "    print('Implementation is correct')\n",
    "else:\n",
    "    print('Oops!! Something is wrong')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 $k$-NN classifier, step by step for a single test example\n",
    "\n",
    "Let's implement the algorithm for **one data sample**, i.e., to classify a single new point.\n",
    "\n",
    "**Q4. You are asked to use the mean and std of the training set to normalize the test data. Can you explain the reasoning behind this?** \n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_sample(data):\n",
    "    \"\"\"Randomly chose a single datapoint from the given data matrix.\n",
    "    \n",
    "    Input:\n",
    "        data: shape (NxD)\n",
    "    Output:\n",
    "        randomly chosen sample: shape (1xD)\n",
    "    \"\"\"\n",
    "    total_samples = data.shape[0]\n",
    "    rand_idx = np.random.choice(total_samples)\n",
    "    return data[rand_idx : rand_idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the functions you defined above to predict the label of a single example-\n",
    "\n",
    "# First, we choose a random test example \n",
    "single_test_data =  choose_random_sample(data_test)\n",
    "\n",
    "# IMPORTANT: Normalize the data, what should the mean and std be?\n",
    "# Hint: mean_val and std_val dimension should be (1x2). Make use of reshape or the argument `keepdims`.\n",
    "\n",
    "# WRITE YOUR CODE HERE\n",
    "mean_val = ...\n",
    "std_val = ...\n",
    "\n",
    "# norm_train is the normalized data_train\n",
    "norm_train = ...\n",
    "\n",
    "# norm_test_single is the normalized single_test_data\n",
    "norm_test_single = ...\n",
    "\n",
    "# set the number of neighbors in the kNN classifier (hyperparameter)\n",
    "# (you can play with that)\n",
    "k = ...\n",
    "\n",
    "# reshape the single example from (1xD) to (D,) for the next function\n",
    "norm_test_single = norm_test_single.reshape(-1)\n",
    "# find distance of the single test example w.r.t. all training examples\n",
    "distances = ...\n",
    "\n",
    "# find the nearest neighbors\n",
    "nn_indices = ...\n",
    "\n",
    "# find the labels of the nearest neighbors\n",
    "neighbor_labels = ...\n",
    "\n",
    "print(\"Nearest {} neighbors' labels: {}\\n\".format(k, \", \".join([class_names[x] for x in neighbor_labels])))\n",
    "\n",
    "# find the best label\n",
    "best_label = predict_label(neighbor_labels) \n",
    "\n",
    "print(f'Predicted label: {class_names[best_label]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the unknown point and its neighbors.\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.title(f\"A randomly chosen unlabeled example from the \"\n",
    "          \"test set\\nand its k-nearest neighbors\")\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*norm_train[labels_train==i].T,\n",
    "                c=colors[i, None], alpha=0.25, \n",
    "                s=20, lw=0, label=class_name)\n",
    "    \n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_indices = nn_indices[labels_train[nn_indices] == i]\n",
    "    if len(class_indices) > 0:\n",
    "        plt.scatter(*norm_train[class_indices].T,\n",
    "                    c=colors[i, None], alpha=1, \n",
    "                    s=25, lw=0, label='Neighbor')\n",
    "\n",
    "# Make sure the norm_test is 1D vector.\n",
    "plt.scatter(*norm_test_single.reshape((-1)), marker='*', c='darkorange', \n",
    "                 alpha=.9, s=75, label='unlabeled example')\n",
    "\n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Performance metrics\n",
    "\n",
    "To quantify the performance of our model, we want to obtain a score that tells us how close the predictions were to the expected classification.\n",
    "\n",
    "The simplest way to do this is to compute the ratio of correctly predicted examples, also known as the accuracy:\n",
    "\n",
    "$$\n",
    "\\frac 1 N \\sum_{n=1}^N \\mathbf{1}[\\hat{y} = y]\n",
    "$$\n",
    "\n",
    "where the indicator function $\\mathbf{1}[\\hat{y} = y]$ returns 1 if the predicted $\\hat{y}$ is equal to the ground-truth $y$ and 0 otherwise.\n",
    "\n",
    "**Q5. Do you see any limitation to using accuracy to evaluate your model?**\n",
    "\n",
    "**Answer.** \n",
    "\n",
    "**Q6. Can you think of other ways to evaluate your model?**\n",
    "\n",
    "**Answer.** \n",
    "\n",
    "**Q7. What other criteria, aside from accuracy, should one consider when choosing hyperparameters?**  \n",
    "\n",
    "**Answer.** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that computes the accuracy between a predicted and the expected labels.\n",
    "def compute_accuracy(predicted, target):\n",
    "    \"\"\"Returns the accuracy score.\n",
    "\n",
    "    Inputs:\n",
    "        predicted: shape (N,) \n",
    "        target: shape (N,) \n",
    "    Outputs:\n",
    "        accuracy\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Putting things together to run the k-NN classifier for all examples\n",
    "\n",
    "Let's implement the algorithm for **all data samples**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function kNN_one_example that applies all the previous steps\n",
    "# to predict the label of 'one' example.\n",
    "\n",
    "def kNN_one_example(unlabeled_example, training_features, training_labels, k):\n",
    "    \"\"\"Returns the label of a single unlabelled example.\n",
    "\n",
    "    Inputs:\n",
    "        unlabeled_example: shape (D,) \n",
    "        training_features: shape (NxD)\n",
    "        training_labels: shape (N,) \n",
    "        k: integer\n",
    "    Outputs:\n",
    "        predicted label\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    \n",
    "    # Compute distances\n",
    "    distances = ...\n",
    "    \n",
    "    # Find neighbors\n",
    "    nn_indices = ...\n",
    "    \n",
    "    # Get neighbors' labels\n",
    "    neighbor_labels = ...\n",
    "    \n",
    "    # Pick the most common\n",
    "    best_label = ...\n",
    "    \n",
    "    return best_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function kNN that applies 'kNN_one_example' function to an arbitrary number of examples.\n",
    "# Tip: NumPy's apply_along_axis does most of the work for you. \n",
    "# It's a one-liner, but you might need to read its documentation!\n",
    "\n",
    "def kNN(unlabeled, training_features, training_labels, k):\n",
    "    \"\"\"Return the labels vector for all unlabeled datapoints.\n",
    "\n",
    "    Inputs:\n",
    "        unlabeled: shape (MxD) \n",
    "        training_features: shape (NxD)\n",
    "        training_labels: shape (N,) \n",
    "        k: integer\n",
    "    Outputs:\n",
    "        predicted labels: shape (M,)\n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE\n",
    "    return np.apply_along_axis(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize your training data and test data\n",
    "# (Don't forget to normalize according to the mean and std of the training set!)\n",
    "\n",
    "mean_val = ...\n",
    "std_val  = ...\n",
    "norm_train_data = ...\n",
    "norm_test_data  = ...\n",
    "\n",
    "# choose a k value (you can play with that)\n",
    "k = ...\n",
    "\n",
    "# run k-NN classifier on complete test data\n",
    "predicted_labels_test = kNN(...)\n",
    "\n",
    "accuracy = ...\n",
    "print(\"Test Accuracy is {:.1f}%\".format(100*accuracy))\n",
    "\n",
    "# Visualize the predictions on the unannotated test set\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Predicted classes for test data\")\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*norm_train_data[labels_train==i].T,\n",
    "                c=colors[i, None], alpha=0.1, s=15, lw=0)\n",
    "    \n",
    "# represent test set by '*' marker\n",
    "for i, class_name in enumerate(class_names):    \n",
    "    plt.scatter(*norm_test_data[predicted_labels_test==i].T,\n",
    "                c=colors[i, None], marker='*', alpha=0.7, \n",
    "                s=50, lw=0, label=class_name)\n",
    "    \n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8. How do you expect the model to perform with large k values equal to the number of training example ?**\n",
    "\n",
    "**Answer.** \n",
    "\n",
    "**Q9. While the above implementation works, it has some drawbacks. Can you identify them?**  \n",
    "\n",
    "**Answer.** \n",
    "\n",
    "**Q10. What should one take into account when feeding new data to a machine learning model?**\n",
    "\n",
    "**Answer.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Written questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** You use $k$-NN on a dataset with $k=21$. You find out that it overfits as performance on the training set is a lot better than on the validation set. What do you expect might help to reduce overfitting: decreasing or increasing the number of neighbors $k$?\n",
    "\n",
    "**A1.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Class imbalance in the training set can be problematic for $k$-NN. Why? What potential solutions could help?\n",
    "\n",
    "**A2.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** (MCQ) Which of the following statements are true for $k$-NN?\n",
    "1. For $k=100$, the time to train $k$-NN is longer than for $k=10$.\n",
    "2. $k$ should be set to the number of classes.\n",
    "3. $k$-NN has _no_ trainable parameters.\n",
    "4. The prediction of the algorithm might change with the distance metric.\n",
    "\n",
    "**A3.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** (SCQ) Let us consider the 2-dimensional training data in the figure below, which contains two classes: \"Positive\" and \"Negative\", denoted by the symbols $+$ and $-$ respectively. Furthermore, let us consider the cosine distance between two points $\\mathrm{u}$ and $\\mathrm{v}$ given by\n",
    "\n",
    "$$ d_{cosine}(\\mathrm{u}, \\mathrm{v}) = 1 - \\frac{\\mathrm{u}^\\top\\mathrm{v}}{\\lVert\\mathrm{u}\\rVert_2 \\cdot \\lVert\\mathrm{v}\\rVert_2} = 1 - \\cos\\theta, $$\n",
    "\n",
    "where $\\theta$ is the angle between the vector $\\mathrm{u}$ and $\\mathrm{v}$.\n",
    "\n",
    "![](img/knn_question.png)\n",
    "\n",
    "Which of the following statement is the correct one if we predict the label of the test point with a $1$-NN?\n",
    "1. Using both cosine and euclidean distances, it is classified as \"Positive\".\n",
    "2. It is classified as \"Positive\" with the cosine distance and \"Negative\" with euclidean distance.\n",
    "3. Using both cosine and euclidean distances, it is classified as \"Negative\".\n",
    "4. It is classified as \"Negative\" with the cosine distance and \"Positive\" with euclidean distance.\n",
    "\n",
    "**A4.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Cross Validation (Optional)\n",
    "\n",
    "How should we be choosing the value of k? If we choose the k that gives us the highest **test** accuracy, we would be **cheating**, because we would be tuning our model to its test performance. \n",
    "\n",
    "In practice, we choose the k that gives us the highest **validation** accuracy, via the cross-validation method. By doing so, we ensure that we select a method which generalizes well to unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.  K-Fold Cross Validation \n",
    "In the course, you saw that we generally reserve a portion of the training data for validation, i.e., testing the performance of our model to choose the hyper-parameter. Below, we introduce \"K-Fold Cross-Validation\".\n",
    "\n",
    "K-fold is a type of cross validation technique that works in the following way:\n",
    "\n",
    "1 - Select a $k$ (do not confuse this with the K-Fold of the cross validation, this is the $k$ for the kNN method!)\n",
    "\n",
    "2 - Split the training data in K equal and disjoint parts\n",
    "\n",
    "3 - Select 1 part as our validation set and the rest as our training set.\n",
    "\n",
    "4 - Train our model on our training set and find the accuracy of the validation set. \n",
    "\n",
    "5 - Repeat steps 3 and 4 K times, each time selecting a different part of the data for the validation set. In the end we will find K different validation accuracies. We will average the validation accuracies and find the validation accuracy that corresponds to the k we chose for kNN. (See the image below).\n",
    "\n",
    "6 - Repeat steps 1-5 (the whole process) for different $k$ values (hyperparameter for kNN). \n",
    "\n",
    "7 - Find the $k$ value that gave the highest validation accuracy. Train your model on the whole training set using this $k$. Test on the test set and report the test accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q. What is the difference between validation set and test set?**  \n",
    "**A.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/cross_validation.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Implementation\n",
    "\n",
    "Now let's begin! We will be doing steps 1-5 of our algorithm above. We will do a K-Fold cross validation to find the validation accuracy for a selected $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFold_cross_validation_KNN(X, Y, K, k):\n",
    "    '''\n",
    "    K-Fold Cross validation function for K-NN\n",
    "\n",
    "    Inputs:\n",
    "        X : training data, shape (NxD)\n",
    "        Y: training labels, shape (N,)\n",
    "        K: number of folds (K in K-fold)\n",
    "        k: number of neighbors for kNN algorithm (the hyperparameter)\n",
    "    Returns:\n",
    "        Average validation accuracy for the selected k.\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    accuracies = []  # list of accuracies\n",
    "    for fold_ind in range(K):\n",
    "        #Split the data into training and validation folds:\n",
    "        \n",
    "        #all the indices of the training dataset\n",
    "        all_ind = np.arange(N)\n",
    "        split_size = N // K\n",
    "        \n",
    "        # Indices of the validation and training examples\n",
    "        val_ind = all_ind[fold_ind * split_size : (fold_ind + 1) * split_size]\n",
    "        ## YOUR CODE HERE (hint: np.setdiff1d is your friend)\n",
    "        train_ind = ...\n",
    "        \n",
    "        X_train_fold = X[train_ind, :]\n",
    "        Y_train_fold = Y[train_ind]\n",
    "        X_val_fold = X[val_ind, :]\n",
    "        Y_val_fold = Y[val_ind]\n",
    "\n",
    "        # Run KNN using the data folds you found above.\n",
    "        # YOUR CODE HERE\n",
    "        Y_val_fold_pred = ...\n",
    "        acc = ...\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    #Find the average validation accuracy over K:\n",
    "    ave_acc = ...\n",
    "    return ave_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a 4-fold cross validation using k-NN, with k=5.\n",
    "\n",
    "K = 4 # 4 fold cross validation\n",
    "k = 5 # number of nearest neighbours for k-NN\n",
    "acc = KFold_cross_validation_KNN(norm_train_data, labels_train, K, k)\n",
    "print(f\"{k}-NN Classifier predicted with average validation accuracy of {acc:.2%}.\")\n",
    "\n",
    "# If your accuracy is not above 80%, you may have made a mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Hyperparameter optimization\n",
    "\n",
    "Now let's find the best $k$! Run the function you wrote above for different values of $k$, and let's see which one gives the best validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the cross validation method with different k values\n",
    "\n",
    "def run_cv_for_hyperparam(X, Y, K, k_list):\n",
    "    '''\n",
    "    K-Fold Cross validation function for K-NN\n",
    "\n",
    "    Inputs:\n",
    "        X : training data, shape (NxD)\n",
    "        Y: training labels, shape (N,)\n",
    "        K: number of folds (K in K-fold)\n",
    "        k: a list of k values for kNN \n",
    "    Returns:\n",
    "        model_performance: a list of validation accuracies corresponding to the k-values     \n",
    "    '''\n",
    "    model_performance = [] \n",
    "    for k in k_list:\n",
    "        #YOUR CODE HERE\n",
    "        ...\n",
    "    return model_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the performances for different values of k\n",
    "\n",
    "# Try these values for hyperparameter k\n",
    "k_list = range(1, 100, 4)          \n",
    "K = 4 # number of fold for K-Fold\n",
    "\n",
    "## YOUR CODE HERE\n",
    "model_performance= run_cv_for_hyperparam(norm_train_data,labels_train, K, k_list)\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.title(\"Performance on the validation set for different values of $k$\")\n",
    "plt.plot(k_list, model_performance)\n",
    "plt.xlabel(\"Number of nearest neighbors $k$\")\n",
    "plt.xticks(k_list)\n",
    "plt.ylabel(\"Performance (accuracy)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick hyperparameter value that yields the best performance\n",
    "# WRITE YOUR CODE HERE\n",
    "best_k = ...\n",
    "\n",
    "print(f\"Best number of nearest neighbors on validation set is k={best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Test accuracies for best model.\n",
    "\n",
    "Now that we have tuned our model, we can apply it for prediction on the test set using the optimal $k$ found on cross-validations set.\n",
    "\n",
    "**Q. How do you expect the model to perform, compared with the cross-validation set performance?**\n",
    "\n",
    "**A.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted labeles for unannotated data.\n",
    "# WRITE YOUR CODE HERE\n",
    "predicted_labels_test = ...\n",
    "accuracy = compute_accuracy(predicted_labels_test, labels_test)\n",
    "print(\"Test Accuracy is {:.1f}%\".format(100*accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the predictions on the unannotated test set\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title(\"Predicted classes for test data\")\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    plt.scatter(*norm_train_data[labels_train==i].T,\n",
    "                c=colors[i, None], alpha=0.1, s=15, lw=0)\n",
    "    \n",
    "# represent test set by '*' marker\n",
    "for i, class_name in enumerate(class_names):    \n",
    "    plt.scatter(*norm_test_data[predicted_labels_test==i].T,\n",
    "                c=colors[i, None], marker='*', alpha=0.7, \n",
    "                s=50, lw=0, label=class_name)\n",
    "    \n",
    "plt.xlabel(\"Weight (normalized)\")\n",
    "plt.ylabel(\"Height (normalized)\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57d8526aca424c3deaf369d9657bf7dc80e220f5e8c5420c16cbdbf3db48769f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
