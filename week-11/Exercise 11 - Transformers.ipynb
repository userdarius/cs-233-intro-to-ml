{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pKMjgAeQO64"
   },
   "source": [
    "# Exercise: Vision Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NnAgI07NeRb"
   },
   "source": [
    "In this exercise session, we will learn about **Vision Transformers**. The Transformer architecture initially emerged in natural language processing as a solution for processing long texts by capturing long-range dependencies. For instance, this is the typical architecture used by Large Language Models like GPT or Gemini. The Vision Transformer, on the other hand, is a variant of the Transformer tailored to process images instead of sequence-like data. While Vision Transformers can outperform CNN architectures on many benchmarks, they may underperform if the amount of data used during training is insufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QZaWQ7d5P9yh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0N8iROw2SzjW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4PEHGe5YoRr"
   },
   "source": [
    "Just like in previous weeks, we will once again be working with the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNBoRin_Ynqb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60000 train and 10000 valid samples.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset_train = MNIST('data', train=True, download=True, transform=ToTensor())\n",
    "dataset_test = MNIST('data', train=False, download=True, transform=ToTensor())\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Loaded {} train and {} valid samples.'.format(len(dataset_train), len(dataset_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vL0cMIqIqZNW"
   },
   "source": [
    "# Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfX1PHF6qcvG"
   },
   "source": [
    "The orignal version of the Vision Transformer (ViT) was introduced in the seminal paper of [Dosovitskiy et al., 2020](https://arxiv.org/abs/2010.11929). It adapts the Transformer architecture to process images for tasks such as image classification. Specifically, it replaces the traditional convolutional layers with self-attention mechanisms, enabling it to capture global dependencies within the image. To accommodate image processing, several additional modules are integrated. Here is an overview ([Bazi et al., 2021](https://www.researchgate.net/publication/348947034_Vision_Transformers_for_Remote_Sensing_Image_Classification)) of the various components of this architecture:\n",
    "\n",
    "![ViT Overview](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tA7xE2dQA_dfzA0Bub5TVw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1-KUjYGNeRf"
   },
   "source": [
    "In this exercise, we will cover each of these modules individually to gain a comprehensive understanding of the entire architecture. Don't hesitate to come back to this figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky3yc4_gZ1Vt"
   },
   "source": [
    "## Patchification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjJ86EyoncBZ"
   },
   "source": [
    "The Transformer architecture operates by treating tokens as input. In textual contexts, each token typically represents either a word or a subword unit. However, in a visual context, tokens are essentially patches extracted from the images. Therefore, the initial step involves dividing the image into several patches of equal size.\n",
    "In the case of the MNIST dataset, we divide the 28x28 images into 7x7 grids, resulting in 49 patches, each measuring 4x4 pixels. It's important to note that since we're working with black-and-white images, there's only a single channel. Following this division, each patch undergoes individual linear projection.\n",
    "\n",
    "![Patchification](https://miro.medium.com/v2/resize:fit:822/format:webp/1*CFbOxEuvo-Pgq7ETIrt0Eg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9B8nYfaZNeRg"
   },
   "source": [
    "Write the code to generate a list of patches for a given set of images. `n_patches` represents the number of patches in a row, set to 7 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDyldGO1Tr5t"
   },
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "    n, c, h, w = images.shape\n",
    "\n",
    "    assert h == w # We assume square image.\n",
    "\n",
    "    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for i in range(n):\n",
    "        # init a counter for the patch index\n",
    "        patch_idx = 0\n",
    "        # loop over the image to extract patches\n",
    "        for row in range(0,h, patch_size):\n",
    "            for col in range(0, w, patch_size):\n",
    "                # extract the patch and flatten it\n",
    "                patch = images[i, :, row:row+patch_size, col:col+patch_size]\n",
    "                patches[i, patch_idx] = patch.flatten()\n",
    "                patch_idx += 1\n",
    "    \n",
    "    return patches\n",
    "\n",
    "\n",
    "assert patchify(torch.rand(2, 1, 28, 28), 2).shape == (2, 4, 196)\n",
    "assert patchify(torch.rand(16, 3, 56, 56), 7).shape == (16, 49, 192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIEcnYYYaAFp"
   },
   "source": [
    "## Positional encoding\n",
    "\n",
    "The positional encoding brings order to the tokens. Without the position token, we do not know where in the image is a particular token from. While there are many different ways to learn the positional encoding, we refer to the earliest implementationm, which used sinusoidals to encode the location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvfIj1nMn8oS"
   },
   "source": [
    "$\n",
    "\\text{positional}_\\text{encoding}(i, 2j) = \\sin\\left(\\frac{{i}}{{10000^{2j/d_{\\text{{model}}}}}}\\right)\n",
    "$\n",
    "\n",
    "$\\text{positional}_\\text{encoding}(i, 2j+1) = \\cos\\left(\\frac{{i}}{{10000^{2j/d_{\\text{{model}}}}}}\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            if j % 2 == 0:\n",
    "                result[i,j] = torch.sin(torch.tensor(i/(10000 ** (j/d))))\n",
    "            else:\n",
    "                result[i, j] = torch.cos(torch.tensor(i/(10000 ** (j/d))))\n",
    "    return result\n",
    "\n",
    "plt.imshow(get_positional_embeddings(49, 128).numpy(), cmap=\"hot\", interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlz7MLHka1Lw"
   },
   "source": [
    "## Multi-head Self-attention\n",
    "\n",
    "The major component in the transformer is the unit of multi-head self-attention mechanism. The transformer views the encoded representation of the input as a set of **key-value** pairs, (**K, V**), both of dimension *n*. The transformer adopts the scaled dot-product attention: the output is a weighted sum of the values, where the weight assigned to each value is determined by the dot-product of the query with all the keys:\n",
    "\n",
    "\n",
    "$$Attention(Q, K, V) = Softmax\\dfrac{QK^T}{\\sqrt{n}}V$$\n",
    "\n",
    "With multi-head self attention, we repeat the attention module multiple times in parallel with the same Q,K,V triplet. The independent attention outputs are simply concatenated and linearly transformed into the expected dimensions. I assume the motivation is because ensembling always helps? ;) According to the paper, “multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
    "\n",
    "![https://lilianweng.github.io/posts/2018-06-24-attention/multi-head-attention.png](https://lilianweng.github.io/posts/2018-06-24-attention/multi-head-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVIQNB24UNnI"
   },
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "    # d is the dimension of the input embedding\n",
    "    # n_heads is the number of head modules (default being 2)\n",
    "    def __init__(self, d, n_heads=2):\n",
    "        super(MyMSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # here we ensure that the input dimension is divisible by the number of heads which is necessary\n",
    "        # since each head will operate on a subspace of the input dimension\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        # calculate the dimension of each head (i.e. how many input values will each head get)\n",
    "        d_head = int(d / n_heads)\n",
    "        self.d_head = d_head\n",
    "\n",
    "        # each input embedding is transformed into three different vectors. \n",
    "        # these transformations allow the model to compute attention scores and derive weighted representations of the input.\n",
    "        # the model learns the weights that transform the d_head input embeddings into the Q K V vectors\n",
    "        # in our case each mapping contains two 2 linear layers \n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "\n",
    "                # Select the mapping associated to the given head.\n",
    "                q_mapping: nn.Linear = self.q_mappings[head]\n",
    "                k_mapping: nn.Linear = self.k_mappings[head]\n",
    "                v_mapping: nn.Linear = self.v_mappings[head]\n",
    "\n",
    "                # the input sequence needs to be split into multiple parts, one for each attention head. \n",
    "                # each head processes a different subspace of the input embedding.\n",
    "                seq: torch.Tensor = sequence[head*self.d_head:(head+1)*self.d_head]\n",
    "\n",
    "                # Map seq to q, k, v.\n",
    "                q, k, v = (q_mapping(seq), k_mapping(seq), v_mapping(seq))\n",
    "\n",
    "                attention = self.softmax((q @ k.T)/torch.sqrt(torch.tensor(self.d_head)))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4D6GUEkbQvQ"
   },
   "source": [
    "## Vit Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3ESIhpkpiva"
   },
   "source": [
    "LayerNorm is a kind of normalization layer which normalizes across all features for a single element in a batch. Given an input, LayerNorm scales the features by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "LayerNorm differs from BatchNorm is the way the statistics are computed. Why BatchNorm scales each feature by computing its statistics across the batch, LayerNorm scales features by computing its statistics across all features for a given element in a batch.\n",
    "\n",
    "More information about various normalisation layers: [YouTube, Yannic Kilcher](https://www.youtube.com/watch?v=l_3zj6HeWUE)\n",
    "\n",
    "\n",
    "![Examples of different normalizations](https://miro.medium.com/v2/resize:fit:656/format:webp/1*xf37Ts0mdBGiS-keOwviOQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3etpUQylUG-4"
   },
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = torch.nn.LayerNorm(hidden_d) \n",
    "        self.mhsa = MyMSA(hidden_d, n_heads) \n",
    "        self.norm2 = torch.nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential( \n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Write code for MHSA + residual connection.\n",
    "        out = self.mhsa(self.norm1(x)) + x\n",
    "        # Write code for MLP(Norm(out)) + residual connection\n",
    "        out = self.mlp(self.norm2(out)) + out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fER1RNuiba4e"
   },
   "source": [
    "## ViT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzkkFa06NeRj"
   },
   "source": [
    "Now we can put all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6whTzSDGTjaD"
   },
   "outputs": [],
   "source": [
    "class MyViT(nn.Module):\n",
    "    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
    "        super(MyViT, self).__init__()\n",
    "\n",
    "        self.chw = chw # input shape : (# of channels, height, width) \n",
    "        self.n_patches = n_patches # number of patches\n",
    "        self.n_blocks = n_blocks # number of transformer blocks\n",
    "        self.n_heads = n_heads # number of heads\n",
    "        self.hidden_d = hidden_d # hidden dimension\n",
    "\n",
    "        # Input and patches sizes\n",
    "        assert chw[1] % n_patches == 0 # Input shape must be divisible by number of patches\n",
    "        assert chw[2] % n_patches == 0\n",
    "        self.patch_size =  (chw[1] / n_patches, chw[2] / n_patches) # size of each patch (height, width)\n",
    "\n",
    "        # Linear mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1]) \n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d) \n",
    "\n",
    "        # Learnable classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # Positional embedding\n",
    "        # HINT: don't forget the classification token\n",
    "        self.positional_embeddings =  get_positional_embeddings(n_patches ** 2 + 1, hidden_d)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "\n",
    "        # Classification MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_d),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "\n",
    "        n, c, h, w = images.shape\n",
    "\n",
    "        # Divide images into patches.\n",
    "        patches = patchify(images, self.n_patches)\n",
    "\n",
    "        # Map the vector corresponding to each patch to the hidden size dimension.\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Add classification token to the tokens.\n",
    "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "\n",
    "        # Add positional embedding.\n",
    "        # HINT: use torch.Tensor.repeat(...)\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "\n",
    "        # Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        # Get the classification token only.\n",
    "        out = out[:, 0]\n",
    "\n",
    "        # Map to the output distribution.\n",
    "        out = self.mlp(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LS8Z-Op6eFIB"
   },
   "outputs": [],
   "source": [
    "chw = (1, 28, 28)\n",
    "n_patches = 7\n",
    "n_blocks = 2\n",
    "n_blocks = 2\n",
    "hidden_d = 8\n",
    "n_heads = 2\n",
    "out_d = 10\n",
    "model = MyViT(chw = (1, 28, 28), n_patches=n_patches, n_blocks=n_blocks,\n",
    "              hidden_d=hidden_d, n_heads=n_heads, out_d=out_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3LFUBlIhP4uQ"
   },
   "outputs": [],
   "source": [
    "def accuracy(x, y):\n",
    "    x = x.detach().cpu().numpy()\n",
    "    y = y.detach().cpu().numpy()\n",
    "    return np.mean(np.argmax(x, axis=1) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfu34Jmiexc0"
   },
   "source": [
    "Now you can reuse the code of last week exercise, since the only difference is the architecture. The training procedure is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TErun42dcivY"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "learning_rate = 0.005\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GIkuzUVTQF4"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloader_train, dataloader_test, epochs):\n",
    "    \"\"\" Trains the model for the specified number of epochs on the dataset.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        criterion: The loss function.\n",
    "        optimizer: The optimizer to use.\n",
    "        dataloader_train: The DataLoader for the training set.\n",
    "        dataloader_test: The DataLoader for the test set.\n",
    "        epochs: The number of epochs to train for.\n",
    "    \"\"\"\n",
    "    for ep in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        for it, batch in enumerate(dataloader_train):\n",
    "\n",
    "            # Load a batch, break it down in images and targets.\n",
    "            x, y = batch\n",
    "            print(x.shape)\n",
    "\n",
    "            # Run forward pass.\n",
    "            logits = model(x)\n",
    "\n",
    "            # Compute loss (using 'criterion').\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            # Run backward pass.\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss += loss.detach().cpu().item() / len(dataloader_train)\n",
    "\n",
    "            # Update the weights using 'optimizer'.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Zero-out the accumulated gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print('\\rEp {}/{}, it {}/{}: loss train: {:.2f}, accuracy train: {:.2f}'.\n",
    "                          format(ep + 1, epochs, it + 1, len(dataloader_train), loss,\n",
    "                                accuracy(logits, y)), end='')\n",
    "\n",
    "        # Validation.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            acc_run = 0\n",
    "            for it, batch in enumerate(dataloader_test):\n",
    "                # Get batch of data.\n",
    "                x, y = batch\n",
    "                curr_bs = x.shape[0]\n",
    "                acc_run += accuracy(model(x), y) * curr_bs\n",
    "            acc = acc_run / len(dataloader_test.dataset)\n",
    "\n",
    "            print(', accuracy test: {:.2f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hP4fXPgRUYTp"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x8 and 4x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vt/t1qn84c15kvd_fh9hr4kvj840000gn/T/ipykernel_10994/501074859.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/vt/t1qn84c15kvd_fh9hr4kvj840000gn/T/ipykernel_10994/3903870105.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, dataloader_train, dataloader_test, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Run forward pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Compute loss (using 'criterion').\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vt/t1qn84c15kvd_fh9hr4kvj840000gn/T/ipykernel_10994/2246385084.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Transformer Blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Get the classification token only.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vt/t1qn84c15kvd_fh9hr4kvj840000gn/T/ipykernel_10994/593367063.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Write code for MHSA + residual connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmhsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Write code for MLP(Norm(out)) + residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vt/t1qn84c15kvd_fh9hr4kvj840000gn/T/ipykernel_10994/1092118220.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# Map seq to q, k, v.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mattention\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x8 and 4x4)"
     ]
    }
   ],
   "source": [
    "train_model(model, criterion, optimizer, dataloader_train, dataloader_test, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sXBRvW-NeRk"
   },
   "source": [
    "How does it compare with the previous project on CNNs?\n",
    "\n",
    "In general Transformers are shown to be effective since they leverage large models with a higher parameter count more effectively than the convolutional architectures. However, the performance gains of transformers may not necessarily replicate when comparing against a small convolutional model. Moreover, this is still an active direction of research, with ConvNext and hybrid approaches claiming at par performance with Transformers.\n",
    "\n",
    "Can you think of different hyperparameters to improve model performance?\n",
    "How would the performance change by modifying the number of heads in multi-head attention? How would performance change by altering the number of patches? Did you try alternate positional encoding techniques?\n",
    "\n",
    "Feel free to tinker with this code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dmvmk7EOp3b"
   },
   "source": [
    "These coding exercises were inspired by this [blog post](https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/zbirobin/iml-week11/blob/main/exercises.ipynb",
     "timestamp": 1715113212813
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
