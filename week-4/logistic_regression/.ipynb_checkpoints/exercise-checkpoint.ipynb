{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise: Logistic Regression\n",
    "\n",
    "This week's exercise is about linear classification, in particular, logistic regression. You will see both the binary and the multi-class variant of the logistic regression."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "# project files\n",
    "import helpers as helpers\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Binary Class Logistic Regression\n",
    "\n",
    "Let's start by loading the [_Iris Flower Dataset_](https://en.wikipedia.org/wiki/Iris_flower_data_set). To facilitate visualization, we will only use 2 out of the 4 features of this dataset. Furthermore, we will use 2 out of the 3 classes in this dataset, named *setosa* and *versicolor*. Therefore, for this part our dataset with two classes is as follows:\n",
    "\n",
    "  - data: $\\mathbf{X} \\in \\mathbb{R}^{N \\times 3}$, $\\forall \\mathbf{x}_i \\in \\mathbf{X}: \\mathbf{x}_i \\in \\mathbb{R}^{3}$ (2 features and the bias)\n",
    "  - labels: $\\mathbf{t} \\in \\mathbb{R}^{N}$, $\\forall t_i \\in \\mathbf{t}: t_i \\in \\{0, 1\\}$ \n",
    "\n",
    "**Note**:\n",
    "\n",
    "**1.** We add here a bias term to our data. To simplify the notation, we directly consider it when we write the number of features $D$, so $D=2+1$ with 2 features and the bias.\n",
    "\n",
    "**2.** $\\mathbf{X}$ is a matrix of shape $N \\times D$. However, a single data sample $\\mathbf{x}_i$ is a column vector of shape $D \\times 1$. \n",
    "To compute a scalar product of one data sample with the weight vector $\\mathbf{w}$ (also a column vector of shape $D \\times 1$), we write $\\mathbf{x}_i^\\top\\mathbf{w}$. To perform a matrix-vector multiplication of the entire data matrix with the weight vector, we write $\\mathbf{X}\\cdot\\mathbf{w}$.\n",
    "\n",
    "\n",
    "**Q.** Verify that the dimensions match in the previous explanation, when using 1) a single data sample $\\mathbf{x}_i$, and 2) the whole data matrix $\\mathbf{X}$.\n",
    "\n",
    "**A.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Loads the data and split them into training and test subsets.\n",
    "data, labels = helpers.load_ds_iris(sep_l=True, sep_w=True, pet_l=False, pet_w=False,\n",
    "                                    setosa=True, versicolor=True, virginica=False, addbias=True)\n",
    "fig = helpers.scatter2d_multiclass(data, labels)\n",
    "\n",
    "num_samples = data.shape[0]\n",
    "\n",
    "fraction_train = 0.8  # 80% of data is reserved for training, so 20% for testing\n",
    "np.random.seed(0)\n",
    "rinds = np.random.permutation(num_samples)  # shuffling of the indices to shuffle the data\n",
    "\n",
    "n_train = int(num_samples * fraction_train)\n",
    "data_train = data[rinds[:n_train]] \n",
    "labels_train = labels[rinds[:n_train]]  \n",
    "\n",
    "data_test = data[rinds[n_train:]] \n",
    "labels_test = labels[rinds[n_train:]]  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 A short introduction\n",
    "\n",
    "In logistic regression, the probability of a datapoint belonging to a class is found as:\n",
    "$$P(y_i=1|\\mathbf{x}_i, \\mathbf{w}) = \\frac{1}{1+e^{-\\mathbf{x}_i^{\\top}\\mathbf{w}}} $$\n",
    "\n",
    "This is called the sigmoid function! The sigmoid function is defined as:\n",
    "$$\\sigma(a)= \\frac{1}{1+e^{-a}}$$\n",
    "\n",
    "So in our case, our model is defined as:\n",
    "$$y(\\mathbf{x}_i; \\mathbf{w})=\\sigma(\\mathbf{x}_i^{\\top}\\mathbf{w})= \\frac{1}{1+e^{-\\mathbf{x}_i^{\\top}\\mathbf{w}}}$$\n",
    "\n",
    "\n",
    "\n",
    "Let's try to code this function. You can use the numpy function `np.exp(x)` to take the exponential of a number."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sigmoid(a):\n",
    "    \"\"\"\n",
    "    Apply the sigmoid function to each element of an array.\n",
    "    \n",
    "    Args:\n",
    "        a (array): Input data of shape (N,) \n",
    "    Returns:\n",
    "        sigmoid(a) (array): Probabilites of shape (N,), where each value is in (0, 1).\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    return ..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TEST YOUR CODE\n",
    "# Is the result what you expected?\n",
    "x = np.arange(-10,10,1)\n",
    "y = sigmoid(x)\n",
    "plt.figure()\n",
    "plt.title(\"Sigmoid function\")\n",
    "plt.plot(x,y); plt.show(); plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall that the cross entropy loss is defined as:\n",
    "$$ \\begin{align}\n",
    "E(\\mathbf{w}) &= -\\sum_i \\left\\{t_i \\ln{y_i} + (1-t_i) \\ln{(1-y_i)}\\right\\} \\\\\n",
    "y_i &= y(\\mathbf{x}_i; \\mathbf{w}) \\end{align}$$\n",
    "\n",
    "Let's code it using NumPy. You can use `np.log(x)` to compute $\\ln{x}$, and the numpy function `np.dot()` or an operator `@` for matrix multiplication. (Recall that the operator `*` is an *element-wise* multiplication in numpy, not the matrix multiplication!)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cross_entropy(data, labels, w): \n",
    "    \"\"\"\n",
    "    Cross-entropy loss for logistic regression on binary classes.\n",
    "    \n",
    "    Args:\n",
    "        data (array): Dataset of shape (N, D).\n",
    "        labels (array): Labels of shape (N,).\n",
    "        w (array): Weights of logistic regression model of shape (D,)\n",
    "    Returns:\n",
    "        float: cross-entropy of the data\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    return ..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TEST YOUR CODE\n",
    "utest_data = np.random.normal(size=(100,5))\n",
    "utest_labels = np.random.binomial(n=1, p=0.5, size=utest_data.shape[0])\n",
    "utest_w = np.random.normal(size=(utest_data.shape[1],))\n",
    "utest_loss = cross_entropy(utest_data, utest_labels, utest_w)\n",
    "print(utest_loss)  # this should print a single number, more or less around ~100\n",
    "\n",
    "assert isinstance(utest_loss, float), \"cross_entropy() does not return the correct format!\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To move the weight vector towards the optimal weights, we need to compute the gradient of the loss function. This gradient is defined as\n",
    "$$\\nabla E(\\mathbf{w}) = \\sum_i (y_i - t_i)\\mathbf{x}_i $$\n",
    "Let us put this into a nice matrix format:\n",
    "$$\\nabla E(\\mathbf{w})= \\mathbf{X}^\\top\\left(\\mathbf{y} - \\mathbf{t}\\right) = \\mathbf{X}^\\top\\left(\\sigma(\\mathbf{X}\\cdot\\mathbf{w}) - \\mathbf{t}\\right),$$\n",
    "\n",
    "where $\\mathbf{y} = \\sigma(\\mathbf{X}\\cdot \\mathbf{w})$ are the predictions, and $\\sigma(\\mathbf{X}\\cdot \\mathbf{w})$ computes the sigmoid for each data sample separately, and returns a vector of shape $(N \\times 1)$.\n",
    "\n",
    "\n",
    "**Q.** What is the shape of $\\nabla E(\\mathbf{w})$? Does it match with the formula above?\n",
    "\n",
    "**A.** \n",
    "\n",
    "Fill in the function for computing the gradient `gradient_cross_entropy()`. You can use the numpy function `np.dot()` or an operator `@` for matrix multiplication."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gradient_cross_entropy(data, labels, w):\n",
    "    \"\"\"\n",
    "    Gradient of the cross-entropy for logistic regression on binary classes.\n",
    "    \n",
    "    Args:\n",
    "        data (array): Dataset of shape (N, D).\n",
    "        labels (array): Labels of shape (N,).\n",
    "        w (array): Weights of logistic regression model of shape (D,)\n",
    "    Returns:\n",
    "        grad (array): Gradient array of shape (D,)\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    return ..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TEST YOUR CODE\n",
    "utest_grad = gradient_cross_entropy(utest_data, utest_labels, utest_w)\n",
    "print(utest_grad)\n",
    "\n",
    "assert (utest_grad.shape == utest_w.shape), \"gradient_cross_entropy() does not return the correct format!\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Classifying using a logistic regression model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us write a function to perform classification using logistic regression, `logistic_regression_predict()`. This function uses the weights we find during training to predict the labels for the data.\n",
    "\n",
    "**Hint:** We classify our data according to $P(y_i=1|\\mathbf{x}_i, \\mathbf{w})$. If the value of $P(y_i=1|\\mathbf{x}_i, \\mathbf{w})$ is less than 0.5 then the data point is classified as label 0. If it is more than or equal to 0.5 then we classify the data point as label 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def logistic_regression_predict(data, w):\n",
    "    \"\"\" \n",
    "    Predict the label of data for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        data (array): Dataset of shape (N, D).\n",
    "        w (array): Weights of logistic regression model of shape (D,)\n",
    "    Returns:\n",
    "        predictions (array): Predicted labels of data, of shape (N,)\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    ...\n",
    "    return predictions.astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "utest_pred = logistic_regression_predict(utest_data, utest_w)\n",
    "print(utest_pred)\n",
    "\n",
    "assert (utest_pred.shape == utest_labels.shape), \"logistic_regression_predict() does not return the correct format!\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We measure the performance of our classifier with the *accuracy* metric. It is defined as \n",
    "\n",
    "$$ \\text{Acc} = \\frac{\\text{\\# correct predictions}}{\\text{\\# all predictions}}\\cdot 100$$\n",
    "\n",
    "Implement the following `accuracy_fn()` function using the predicted and ground truth labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def accuracy_fn(labels_pred, labels_gt):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the predictions (in percent).\n",
    "    \n",
    "    Args:\n",
    "        labels_pred (array): Predicted labels of shape (N,)\n",
    "        labels_gt (array): GT labels of shape (N,)\n",
    "    Returns:\n",
    "        acc (float): Accuracy, in range [0, 100].\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    return ..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "utest_labels1 = np.random.binomial(n=1, p=0.5, size=10)\n",
    "utest_labels2 = np.random.binomial(n=1, p=0.5, size=10)                \n",
    "utest_acc = accuracy_fn(utest_labels1, utest_labels2)\n",
    "print(utest_acc)\n",
    "\n",
    "assert isinstance(utest_acc, float) and (0 <= utest_acc <= 100), \"accuracy_fn() does not return the correct format!\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Training a logistic regression model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To find the optimal weights for the given training data, we need to train our model. Fill in the missing parts of the function `logistic_regression_train()`.\n",
    "\n",
    "The function first initializes the weights randomly (according to a Gaussian distribution). In each iteration, you should compute the gradient using `gradient_cross_entropy()` and take a gradient step to update the weights. Given that $\\eta$ is the learning rate, recall that a gradient step is expressed as: \n",
    "\n",
    "$$ \\mathbf{w}_{[t + 1]}  = \\mathbf{w}_{[t]} - \\eta\\, \\nabla E\\left(\\mathbf{w}_{[t]}\\right) $$\n",
    "\n",
    "The `loss`, `plot` and `print_every` parameters affect the way the loss is printed and the predictions are displayed. You do not need to modify these parts.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def logistic_regression_train(data, labels, max_iters=10, lr=0.001, \n",
    "                              print_period=1000, plot_period=1000):\n",
    "    \"\"\"\n",
    "    Training function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        data (array): Dataset of shape (N, D).\n",
    "        labels (array): Labels of shape (N,).\n",
    "        max_iters (int): Maximum number of iterations. Default: 10\n",
    "        lr (int): The learning rate of  the gradient step. Default: 0.001\n",
    "        print_period (int): Number of iterations to print current loss. \n",
    "            If 0, never printed.\n",
    "        plot_period (int): Number of iterations to plot current predictions.\n",
    "            If 0, never plotted.\n",
    "    Returns:\n",
    "        weights (array): weights of the logistic regression model, of shape(D,)\n",
    "    \"\"\"\n",
    "    # Initialize the weights randomly according to a Gaussian distribution\n",
    "    weights = np.random.normal(0., 0.1, [data.shape[1],])\n",
    "    for it in range(max_iters):\n",
    "        ############# WRITE YOUR CODE HERE: find the gradient and do a gradient step\n",
    "        gradient = ...\n",
    "        weights = ...\n",
    "        ##################################\n",
    "        \n",
    "        # If we reach 100% accuracy, we can stop training immediately\n",
    "        predictions = logistic_regression_predict(data, weights)\n",
    "        if accuracy_fn(predictions, labels) == 100:\n",
    "            break\n",
    "        # logging\n",
    "        if print_period and it % print_period == 0:\n",
    "            print('loss at iteration', it, \":\", cross_entropy(data, labels, weights))\n",
    "        # plotting\n",
    "        if plot_period and it % plot_period == 0:\n",
    "            fig = helpers.visualize_predictions(data=data, labels_gt=labels, labels_pred=predictions, title=\"iteration \"+ str(it))\n",
    "        \n",
    "    fig = helpers.visualize_predictions(data=data, labels_gt=labels, labels_pred=predictions, title=\"final model\")\n",
    "    return weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the code below to see your training in action. What do you observe? Try playing with the learning rate and number of max iterations.\n",
    "\n",
    "*Hint:* we usually try different magnitude for the learning rate, such as 0.01, 0.001, 0.0001, etc.\n",
    "\n",
    "If the code is very slow, verify if you used loops instead of leveraging numpy's efficient array computations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weights = logistic_regression_train(data_train, labels_train, max_iters=100000, lr=1e-2, print_period=1000, plot_period=3000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now use this trained model to make prediction on test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions = logistic_regression_predict(data_test, weights)\n",
    "fig = helpers.visualize_predictions(data=data_test, labels_gt=labels_test, labels_pred=predictions, title=\"test results\")\n",
    "print(f\"Test accuracy is {accuracy_fn(predictions, labels_test)}%\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have classified two classes, we can move on to multi-class logistic regression!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Multi-Class Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the synthetic data by running the code segment below. We will use this dataset for now as it is easy to work with.\n",
    "Our data is:\n",
    "\n",
    "  - data: $\\mathbf{X} \\in \\mathbb{R}^{N \\times 3}$, $\\forall \\mathbf{x}_i \\in \\mathbf{X}: \\mathbf{x}_i \\in \\mathbb{R}^{3}$ (2 features and the bias)\n",
    "  - labels: $\\mathbf{T} \\in \\mathbb{R}^{N\\times C}$, $\\forall \\mathbf{t}_i \\in \\mathbf{T}: \\mathbf{t}_i \\in \\mathbb{R}^{C}$ is a one-hot encoding of the label of a data sample, with $C$ the number of classes. $t_i^k$ is $1$ if $\\mathbf{x}_i$ is of class $k$, otherwise $t_i^k=0$.\n",
    "\n",
    "**A word on one-hot encoding:** the true labels $t_i\\in\\mathbf{t}$ can be integers, representing the class number in $\\{0, 1, ..., C-1\\}$, or they can be what we call *one-hot encoded*. This encoding consists in replacing the class number by a vector of dimension $C$ that is all $0$ except for the dimension that corresponds to the class where it is $1$. Let's give a few examples to explain: (considering $C=5$ classes)\n",
    "* $t_i=0$ would become $\\mathbf{t}_i = [1, 0, 0, 0, 0]^\\top$\n",
    "* $t_i=2$ would become $\\mathbf{t}_i = [0, 0, 1, 0, 0]^\\top$\n",
    "* $t_i=4$ would become $\\mathbf{t}_i = [0, 0, 0, 0, 1]^\\top$\n",
    "\n",
    "Such an encoding is both useful for simplifying some of the equations we use and has a nice geometric property: each class in this $C$-dimensional space is orthogonal and as \"distant\" to each other. With the integer representation, we have an order on the classes such as class $1$ is closer to class $0$ than class $2$ that may not have any meaning. The one-hot encoding removes such superfluous ordering."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_multi, labels_multi = helpers.load_dataset_synth(addbias=True)\n",
    "fig = helpers.scatter2d_multiclass(data_multi, helpers.onehot_to_label(labels_multi), fig=None, fig_size=None, color_map=None,\n",
    "                                   legend=True, legend_map=None, grid=False, show=False)\n",
    "\n",
    "num_samples = data_multi.shape[0]\n",
    "\n",
    "fraction_train = 0.8  # 80% of data is reserved for training, so 20% for testing\n",
    "np.random.seed(0)\n",
    "rinds = np.random.permutation(num_samples)  # shuffling of the indices to shuffle the data\n",
    "\n",
    "n_train = int(num_samples * fraction_train)\n",
    "data_train = data_multi[rinds[:n_train]] \n",
    "labels_train = labels_multi[rinds[:n_train]]  \n",
    "\n",
    "data_test = data_multi[rinds[n_train:]] \n",
    "labels_test= labels_multi[rinds[n_train:]]  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 A short introduction \n",
    "\n",
    "Multi-class logistic regression is an extension to binary logistic regression.\n",
    "\n",
    "Let us consider logistic regression for $C$ classes. We keep our weights in a weight matrix $\\mathbf{W}$, where every column is $\\mathbf{w}_{(k)}$ for class $k$. Therefore, for every class $k$, we learn a separate weight vector $\\mathbf{w}_{(k)}$ during training. The weights matrix will be of size $D \\times C$.\n",
    "\n",
    "The generalized probabilities for logistic regression are\n",
    "\n",
    "$$y^{(k)}(\\mathbf{x}_i) = P(y_i=k|\\mathbf{x}_i, \\mathbf{W}) = \\frac{\\exp{\\mathbf{x}_i^\\top \\mathbf{w}_{(k)}}}{\\sum_j^C \\exp{\\mathbf{x}_i^\\top\\mathbf{w}_{(j)}}},$$ \n",
    "\n",
    "which is called the **softmax** function! Let us denote this function by $f_\\text{softmax}$. This is sort of an extension of the sigmoid function for the multi-class setting.\n",
    "\n",
    "Fill in the implementation of this function below. It is used to assign the probabilities of a datapoint belonging to each class. For example, for a single datapoint and 3 classes you might have the following probability assignments: {0.2, 0.7, 0.1}. The probabilities all sum up to 1. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def f_softmax(data, W):\n",
    "    \"\"\"\n",
    "    Softmax function for multi-class logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        data (array): Input data of shape (N, D)\n",
    "        W (array): Weights of shape (D, C) where C is the number of classes\n",
    "    Returns:\n",
    "        array of shape (N, C): Probability array where each value is in the\n",
    "            range [0, 1] and each row sums to 1.\n",
    "            The row i corresponds to the prediction of the ith data sample, and \n",
    "            the column j to the jth class. So element [i, j] is P(y_i=k | x_i, W)\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE \n",
    "    # Hint: try to decompose the above formula in different steps to avoid recomputing the same things.\n",
    "    return ..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TEST YOUR CODE\n",
    "utest_data = np.random.normal(size=(100,5))\n",
    "utest_w = np.random.normal(size=(5,4))\n",
    "utest_softmax = f_softmax(utest_data, utest_w)\n",
    "print(utest_softmax.shape)\n",
    "\n",
    "assert (utest_softmax.shape == (utest_data.shape[0], utest_w.shape[1])), \"f_softmax() does not return the correct format!\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using these, we find the loss function which we are trying to minimize is\n",
    "\n",
    "$$ E(\\mathbf{W}) = - \\sum_{i}^N \\sum_{k}^N t_i^{(k)} \\ln{\\left(y^{(k)}(\\mathbf{x}_i)\\right)}, $$\n",
    "\n",
    "where $y^{(k)}(\\mathbf{x}_i)$ is the predicted probability of data sample $\\mathbf{x}_i$ for class $k$, after using the *softmax*.\n",
    "\n",
    "Fill in the loss function below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def loss_logistic_multi(data, labels, w):\n",
    "    \"\"\" \n",
    "    Loss function for multi class logistic regression, i.e., multi-class entropy.\n",
    "    \n",
    "    Args:\n",
    "        data (array): Input data of shape (N, D)\n",
    "        labels (array): Labels of shape  (N, C)  (in one-hot representation)\n",
    "        w (array): Weights of shape (D, C)\n",
    "    Returns:\n",
    "        float: Loss value \n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE \n",
    "    return ..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TEST YOUR CODE\n",
    "utest_labels = helpers.label_to_onehot(np.random.binomial(n=utest_w.shape[1]-1, p=0.5, size=100))\n",
    "utest_loss = loss_logistic_multi(utest_data, utest_labels, utest_w)\n",
    "print(utest_loss)  # this should print a single number, more or less around ~250\n",
    "\n",
    "assert isinstance(utest_loss, float), \"loss_logistic_multi() does not return the correct format!\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To find the gradient, we find the gradient of $E(\\mathbf{W})$ with respect to the weights $\\mathbf{W}$. We have for each class weight $\\mathbf{w}^{(k)}$:\n",
    "\n",
    "$$\\nabla E(\\mathbf{w}^{(k)})=\\sum_i^N \\left(y^{(k)}(\\mathbf{x}_i) - t_i^{(k)}\\right) \\mathbf{x}_{i},$$\n",
    "\n",
    "or considering the full weight matrix $\\mathbf{W}$:\n",
    "\n",
    "$$\\nabla E(\\mathbf{W})=\\sum_i^N \\mathbf{x}_{i} \\left(\\mathbf{y}(\\mathbf{x}_i) - \\mathbf{t}_i\\right)^\\top ,$$\n",
    "\n",
    "Let's put this into matrix format as well:\n",
    "\n",
    "$$\\nabla E(\\mathbf{W})= \\mathbf{X}^T(\\mathbf{y}(\\mathbf{X}) - \\mathbf{T})$$\n",
    "\n",
    "**A note on the notation:**   \n",
    "Here, $\\mathbf{y}(\\mathbf{x}_i)$ returns the softmax result of shape $(C \\times 1)$ for sample $\\mathbf{x}_i$ and all classes. \n",
    "\n",
    "$\\mathbf{y}(\\mathbf{X})$ should return a matrix of shape $(N\\times C)$, which consists of the softmax predictions for every sample for all classes. (The $i^{\\mathrm{th}}$ row of $\\mathbf{y}(\\mathbf{X})$ is $\\mathbf{y}(\\mathbf{x}_i)$.)\n",
    "\n",
    "Now, you will fill in the gradient function, `gradient_logistic_multi()` given below. \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gradient_logistic_multi(data, labels, W):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the entropy for multi-class logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        data (array): Input data of shape (N, D)\n",
    "        labels (array): Labels of shape  (N, C)  (in one-hot representation)\n",
    "        W (array): Weights of shape (D, C)\n",
    "    Returns:\n",
    "        grad (np.array): Gradients of shape (D, C)\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE \n",
    "    return ..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "utest_grad = gradient_logistic_multi(utest_data, utest_labels, utest_w)\n",
    "print(utest_grad.shape, \"\\n\", utest_grad)\n",
    "\n",
    "assert (utest_grad.shape == utest_w.shape), \"gradient_logistic_multi() does not return the correct format!\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Classification and training for multiple classes\n",
    "\n",
    "Write the functions for classification (predictions) and training.\n",
    "\n",
    "Hints:\n",
    "* For the classification function, you will be using $f_\\text{softmax}$ to assign the probabilities of a datapoint belonging to each class. The softmax function returns an array of size $(N \\times C)$.\n",
    "* You will have to convert one-hot representation to labels (`np.argmax` is your friend). \n",
    "\n",
    "* Training will be the same as the binary case. First, we will find the gradient. Then we will update the weights using gradient descent."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def logistic_regression_predict_multi(data, W):\n",
    "    \"\"\"\n",
    "    Prediction the label of data for multi-class logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        data (array): Dataset of shape (N, D).\n",
    "        W (array): Weights of multi-class logistic regression model of shape (D, C)\n",
    "    Returns:\n",
    "        array of shape (N,): Label predictions of data.\n",
    "    \"\"\"\n",
    "    ### WRITE YOUR CODE HERE\n",
    "    return ..."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def logistic_regression_train_multi(data, labels, max_iters=10, lr=0.001, \n",
    "                                    print_period=5, plot_period=5):\n",
    "    \"\"\"\n",
    "    Training function for multi class logistic regression.\n",
    "    \n",
    "    Args:\n",
    "        data (array): Dataset of shape (N, D).\n",
    "        labels (array): Labels of shape (N, C)\n",
    "        max_iters (int): Maximum number of iterations. Default: 10\n",
    "        lr (int): The learning rate of  the gradient step. Default: 0.001\n",
    "        print_period (int): Number of iterations to print current loss. \n",
    "            If 0, never printed.\n",
    "        plot_period (int): Number of iterations to plot current predictions.\n",
    "            If 0, never plotted.\n",
    "    Returns:\n",
    "        weights (array): weights of the logistic regression model, of shape(D, C)\n",
    "    \"\"\"\n",
    "    D = data.shape[1]  # number of features\n",
    "    C = labels.shape[1]  # number of classes\n",
    "    # Random initialization of the weights\n",
    "    weights = np.random.normal(0, 0.1, (D, C))\n",
    "    for it in range(max_iters):\n",
    "        ### WRITE YOUR CODE HERE\n",
    "        ...\n",
    "        ##################################\n",
    "\n",
    "        predictions = logistic_regression_predict_multi(data, weights)\n",
    "        if accuracy_fn(predictions, helpers.onehot_to_label(labels)) == 100:\n",
    "            break\n",
    "        #logging and plotting\n",
    "        if print_period and it % print_period == 0:\n",
    "            print('loss at iteration', it, \":\", loss_logistic_multi(data, labels, weights))\n",
    "        if plot_period and it % plot_period == 0:\n",
    "            fig = helpers.visualize_predictions(data=data, labels_gt=helpers.onehot_to_label(labels), labels_pred=predictions, title=\"iteration \"+ str(it))\n",
    "            \n",
    "    fig = helpers.visualize_predictions(data=data, labels_gt=helpers.onehot_to_label(labels), labels_pred=predictions, title=\"final model\")\n",
    "    return weights"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the code below to train the logistic regression model. What do you observe? Try playing with the learning rate and number of max iterations.\n",
    "\n",
    "If the code is very slow, verify if you used loops instead of leveraging numpy's efficient array computations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weights_multi = logistic_regression_train_multi(data_train, labels_train, max_iters=20, lr=1e-3, print_period=5, plot_period=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, let's predict the test data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictions_multi = logistic_regression_predict_multi(data_test, weights_multi)\n",
    "fig = helpers.visualize_predictions(data=data_test, labels_gt=helpers.onehot_to_label(labels_test), labels_pred=predictions_multi, title=\"test result\")\n",
    "print(f\"Test accuracy is {accuracy_fn(predictions_multi, helpers.onehot_to_label(labels_test))}%.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**An (optional) side note:** Notice that using this simple formulation, we have trained C classifiers for C classes.\n",
    "Our probability assignments are according to the softmax function.\n",
    "\n",
    "$$P(\\mathbf{y}_i=k|\\mathbf{x}_i, \\mathbf{W}) = \\frac{\\exp{\\mathbf{x}_i^\\top \\mathbf{w}_{(k)}}}{\\sum_j^C \\exp{\\mathbf{x}_i^\\top\\mathbf{w}_{(j)}}}$$\n",
    "\n",
    "And \n",
    "\n",
    "$$\\sum_{k}^{C} P(\\mathbf{y}_i=k|\\mathbf{x}_i, \\mathbf{W}) = 1$$\n",
    "\n",
    "However, in the binary case we were training $1$ classifier for $2$ classes. The probabilities are assigned according to the sigmoid function:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(\\mathbf{y}_i=1|\\mathbf{x}_i, \\mathbf{w}) &= \\frac{1}{1+\\exp{-\\mathbf{x}_i^\\top \\mathbf{w}}} \\\\\n",
    "P(\\mathbf{y}_i=0|\\mathbf{x}_i, \\mathbf{w}) &= 1-P(\\mathbf{y}_i=1|\\mathbf{x}_i, \\mathbf{w}) = \\frac{1}{1+\\exp{\\mathbf{x}_i^\\top \\mathbf{w}}}\n",
    "\\end{align}$$\n",
    "\n",
    "Similar to the binary case, we can train $C-1$ classifiers for $C$ classes, instead of the $C$ we trained above. We modify the probability assignment function to be, for classes $k={1, ... ,C-1}$:\n",
    "\n",
    "$$ P(\\mathbf{y}_i=k|\\mathbf{x}_i, \\mathbf{W}) = \\frac{\\exp{\\mathbf{x}_i^\\top \\mathbf{w}_{(k)}}}{1+\\sum_j^{C-1} \\exp{\\mathbf{x}_i^\\top \\mathbf{w}_{(j)}}}. $$\n",
    "\n",
    "**Q (optional).** What is $P(\\mathbf{y}_i=0|\\mathbf{x}_i, \\mathbf{W})$?\n",
    "\n",
    "**A.** \n",
    "\n",
    "Further reading: https://en.wikipedia.org/wiki/Multinomial_logistic_regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Written questions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q1.** (MCQ) Considering logistic regression, select all of the correct statements below.\n",
    "1. The cross-entropy loss is non convex with respect to the weight $\\mathbf{w}$.\n",
    "2. Logistic regression is generally more robust to outliers than linear regression for classification.\n",
    "3. Logistic regression can be used to predict the value of someone's weight (in kilograms) based on their height (in meters).\n",
    "4. For a binary classification problem, the value of $\\sigma(\\mathbf{x}^\\top \\mathbf{w})$, with $\\sigma$ the sigmoid function, can be interpreted as the probability that $\\mathbf{x}$ belongs to class $1$.\n",
    "\n",
    "**A1.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q2.** Considering gradient descent on the cross-entropy $E(\\mathbf{w})$ for logistic regression:   \n",
    "a) Recall what is the formula to update the value of $\\mathbf{w}$ using the gradient $\\nabla E(\\mathbf{w})$ without looking above.   \n",
    "b) Generally speaking, what is gradient descent used for?   \n",
    "c) What is the impact of $\\eta$, the learning rate (too small or too large)?\n",
    "\n",
    "**A2.** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q3.** You have the following datasets with data points $\\mathbf{x}\\in\\mathbb{R}^2$ and labels $y\\in\\{0,1\\}$. The points of class $0$ are the circles, while points of class $1$ are the triangles. The filled points represent the train set while the empty ones are the test set.\n",
    "\n",
    "<img src=\"img/logreg_q3.png\" width=100%>\n",
    "\n",
    "For each dataset, how well do you expect logistic regression to perform on the test data? Why?\n",
    "\n",
    "**A3.** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Q4.** (MCQ) Recall that the cross-entropy loss, used for multi-class logistic   regression, is defined as:\n",
    "$$\\begin{align}\n",
    "    E(\\mathbf{W}) = -\\sum_{i=1}^N \\sum_{k=1}^C t^{(k)}_i \\ln y^{(k)}(\\mathbf{x}_i),\n",
    "\\end{align}$$\n",
    "\n",
    "where $N$ is the number of samples, $C$ is the number of classes, $\\mathbf{x}_i$ is a data sample, $\\mathbf{t}_i$ is a label (one-hot encoded) and $y^{(k)}(\\mathbf{x}_i)$ is the predicted probability of sample $\\mathbf{x}_i$ belonging to class $k$.\n",
    "\n",
    "Which of the following statements are true for the cross-entropy loss?\n",
    "\n",
    "1. The prediction $y(\\mathbf{x}_i)$ is found by applying the softmax function to the output of a linear model $\\mathbf{W}\\mathbf{x}_i$.\n",
    "2. It is not a differentiable loss function, therefore we cannot use gradient descent.\n",
    "3. Since it does not have a closed form solution, we have to use an iterative optimization method such as gradient descent.\n",
    "4. The loss is $0$ for samples where the label $t^{(k)}$ and the prediction $y^{(k)}$ are the same.\n",
    "5. If sample $i$ belongs to class $2$, the loss is higher when the prediction is $\\mathbf{y}_i = \\begin{bmatrix}0&0&0&0&1\\end{bmatrix}$(predicted as class $4$) than when the prediction is $\\begin{bmatrix}0&1&0&0&0\\end{bmatrix}$ (predicted as class $1$).\n",
    "6. $\\sum_{k=1}^C y^{(k)}(\\mathbf{x}_i)=1$\n",
    "\n",
    "**A4.** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.16 64-bit ('introml': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "interpreter": {
   "hash": "57d8526aca424c3deaf369d9657bf7dc80e220f5e8c5420c16cbdbf3db48769f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}